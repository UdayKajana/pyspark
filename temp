import argparse
import logging
from typing import NamedTuple, Iterator, Optional
import apache_beam as beam
import apache_beam.io.gcp.spanner as sp
from apache_beam.options.pipeline_options import PipelineOptions
from apache_beam.io.gcp.bigquery import ReadFromBigQuery
from datetime import datetime, date
from decimal import Decimal
from apache_beam.metrics import Metrics, MetricsFilter
import time

class MetricsCollector:
    def __init__(self):
        # Counter metrics
        self.total_records = Metrics.counter('main', 'total_records')
        self.successful_insertions = Metrics.counter('main', 'successful_insertions') 
        self.failed_insertions = Metrics.counter('main', 'failed_insertions')
        self.records_processed = Metrics.counter('main', 'records_processed')
        
        # Distribution metrics for latencies (in milliseconds)
        self.processing_latency = Metrics.distribution('latency', 'processing_ms')
        
        # Gauge metrics for monitoring progress
        self.current_timestamp = Metrics.gauge('progress', 'current_timestamp')

class TemplateOptions(PipelineOptions):
    @classmethod
    def _add_argparse_args(cls, parser):
        parser.add_value_provider_argument('--query', type=str)

class WlnModelScoresMutation(NamedTuple):
    acct_sk: Optional[int]
    acct_type_cd: Optional[str]
    create_date: Optional[str]
    epsilon_cust_id: Optional[int]
    insert_ts: Optional[str]
    model_centile: Optional[int]
    model_decile: Optional[int]
    model_id: Optional[str]
    model_score: Optional[Decimal]
    model_subsegment: Optional[str]
    model_version: Optional[int]
    round_cycle: Optional[int]
    scoring_driver_1: Optional[str]
    scoring_driver_2: Optional[str]
    scoring_driver_3: Optional[str]
    insertion_timestamp: Optional[str]

beam.coders.registry.register_coder(WlnModelScoresMutation, beam.coders.RowCoder)

class MeasureLatencyFn(beam.DoFn):
    def __init__(self, operation_name):
        self.operation_name = operation_name
        self.latency = Metrics.distribution('latency', f'{operation_name}_ms')
    
    def process(self, element):
        start_time = time.time()
        yield element
        self.latency.update((time.time() - start_time) * 1000)

class PrepareDataWithMetrics(beam.DoFn):
    def setup(self):
        self.metrics = MetricsCollector()

    def process(self, element) -> Iterator[WlnModelScoresMutation]:
        start_time = time.time()
        try:
            self.metrics.total_records.inc()
            
            # Update progress gauge
            current_time = time.time()
            self.metrics.current_timestamp.set(int(current_time * 1000))
            
            result = self.transform_to_mutation(element)
            self.metrics.records_processed.inc()
            self.metrics.successful_insertions.inc()
            
            processing_time = (time.time() - start_time) * 1000
            self.metrics.processing_latency.update(processing_time)
            
            yield result
            
        except Exception as e:
            self.metrics.failed_insertions.inc()
            logging.error(f"Error processing element: {element}")
            logging.error(f"Error details: {str(e)}")
            raise

    def format_date(self, value):
        """Format date or datetime objects to string."""
        if value is None:
            return None
        if isinstance(value, (date, datetime)):
            return value.strftime('%Y-%m-%d')
        return value

    def format_timestamp(self, value):
        if value is None:
            return None
        if isinstance(value, datetime):
            return value.strftime('%Y-%m-%dT%H:%M:%S.%fZ')
        if isinstance(value, date):
            return f"{value.strftime('%Y-%m-%d')}T00:00:00.000000Z"
        return value

    def transform_to_mutation(self, row):
        create_date = self.format_date(row.get('create_date'))
        insert_ts = self.format_timestamp(row.get('insert_ts'))
        model_score = row.get('model_score')
        if model_score is not None:
            model_score = Decimal(str(model_score)).quantize(Decimal('1e-9'))
        current_time = datetime.utcnow().strftime('%Y-%m-%dT%H:%M:%S.%fZ')
        
        return WlnModelScoresMutation(
            acct_sk=row.get('acct_sk'),
            acct_type_cd=row.get('acct_type_cd'),
            create_date=create_date,
            epsilon_cust_id=row.get('epsilon_cust_id'),
            insert_ts=insert_ts,
            model_centile=row.get('model_centile'),
            model_decile=row.get('model_decile'),
            model_id=row.get('model_id'),
            model_score=round(model_score, 9) if model_score is not None else None,
            model_subsegment=row.get('model_subsegment'),
            model_version=row.get('model_version'),
            round_cycle=row.get('round_cycle'),
            scoring_driver_1=row.get('scoring_driver_1'),
            scoring_driver_2=row.get('scoring_driver_2'),
            scoring_driver_3=row.get('scoring_driver_3'),
            insertion_timestamp=current_time
        )

def run(args, beam_args):
    options = {
        'project': args.project,
        'runner': args.runner,
        'region': args.region,
        'staging_location': args.staging_location,
        'temp_location': args.temp_location,
        'template_location': args.template_location,
        'save_main_session': True,
        'streaming': False
    }

    pipeline_options = PipelineOptions.from_dictionary(options)
    template_options = pipeline_options.view_as(TemplateOptions)

    with beam.Pipeline(options=pipeline_options) as p:
        # Read from BigQuery with accurate per-element latency tracking
        data = (p 
               | "ReadFromBigQuery" >> ReadFromBigQuery(
                   query=template_options.query,
                   use_standard_sql=True)
               | "MeasureBQLatency" >> beam.ParDo(MeasureLatencyFn("bigquery_read")))

        processed_data = (data 
                        | 'PrepareData' >> beam.ParDo(PrepareDataWithMetrics())
                        .with_output_types(WlnModelScoresMutation))

        # Write to Spanner with accurate per-element latency tracking
        _ = (processed_data 
             | "MeasureSpannerLatency" >> beam.ParDo(MeasureLatencyFn("spanner_write"))
             | "WriteToSpanner" >> sp.SpannerInsertOrUpdate(
                 project_id=args.spanner_project,
                 instance_id=args.spanner_instance,
                 database_id=args.spanner_dataset,
                 table=args.spanner_table,
                 expansion_service=args.expansion_service))

def parse_arguments():
    parser = argparse.ArgumentParser()
    parser.add_argument('--project', required=True, help='Project ID')
    parser.add_argument('--runner', default="DataflowRunner", help='Pipeline runner')
    parser.add_argument('--region', default="us-east4", help='GCP project region')
    parser.add_argument('--staging_location', required=True, help='Staging GCS bucket path')
    parser.add_argument('--temp_location', required=True, help='Temp GCS bucket path')
    parser.add_argument('--template_location', required=True, help='GCS bucket path to save template')
    parser.add_argument('--spanner_dataset', required=True, help='spanner dataset')
    parser.add_argument('--spanner_instance', required=True, help='spanner instance')
    parser.add_argument('--spanner_project', required=True, help='spanner project')
    parser.add_argument('--spanner_table', required=True, help='spanner table')
    parser.add_argument('--expansion_service', default='localhost:50768', help='Expansion service host:port')
    return parser.parse_known_args()

if __name__ == '__main__':
    logging.getLogger().setLevel(logging.INFO)
    known_args, beam_args = parse_arguments()
    run(known_args, beam_args)
    print(f'Created template at {known_args.template_location}.')
