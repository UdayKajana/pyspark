import argparse
import logging
from typing import NamedTuple, Iterator, Optional
import apache_beam as beam
import apache_beam.io.gcp.spanner as sp
from apache_beam.options.pipeline_options import PipelineOptions
from apache_beam.io.gcp.bigquery import ReadFromBigQuery
from datetime import datetime, date
from decimal import Decimal

class TemplateOptions(PipelineOptions):
    @classmethod
    def _add_argparse_args(cls, parser):
        parser.add_value_provider_argument('--query', type=str)

class WlnModelScoresMutation(NamedTuple):
    acct_sk: Optional[int]
    acct_type_cd: Optional[str]
    create_date: Optional[str]
    epsilon_cust_id: Optional[int]
    insert_ts: Optional[str]
    model_centile: Optional[int]
    model_decile: Optional[int]
    model_id: Optional[str]
    model_score: Optional[Decimal]
    model_subsegment: Optional[str]
    model_version: Optional[int]
    round_cycle: Optional[int]
    scoring_driver_1: Optional[str]
    scoring_driver_2: Optional[str]
    scoring_driver_3: Optional[str]
    insertion_timestamp: Optional[str]

beam.coders.registry.register_coder(WlnModelScoresMutation, beam.coders.RowCoder)

class PrepareData(beam.DoFn):
    def process(self, element) -> Iterator[WlnModelScoresMutation]:
        try:
            yield self.transform_to_mutation(element)
        except Exception as e:
            logging.error(f"Error processing element: {element}")
            logging.error(f"Error details: {str(e)}")
            raise

    def format_date(self, value):
        """Format date or datetime objects to string."""
        if value is None:
            return None
        if isinstance(value, (date, datetime)):
            return value.strftime('%Y-%m-%d')
        return value

    def format_timestamp(self, value):
        if value is None:
            return None
        if isinstance(value, datetime):
            return value.strftime('%Y-%m-%dT%H:%M:%S.%fZ')
        if isinstance(value, date):
            return f"{value.strftime('%Y-%m-%d')}T00:00:00.000000Z"
        return value

    def transform_to_mutation(self, row):
        create_date = self.format_date(row.get('create_date'))
        insert_ts = self.format_timestamp(row.get('insert_ts'))
        model_score = row.get('model_score')
        if model_score is not None:
            model_score = Decimal(str(model_score)).quantize(Decimal('1e-9'))
        current_time = datetime.utcnow().strftime('%Y-%m-%dT%H:%M:%S.%fZ')
        return WlnModelScoresMutation(
            acct_sk=row.get('acct_sk'),
            acct_type_cd=row.get('acct_type_cd'),
            create_date=create_date,
            epsilon_cust_id=row.get('epsilon_cust_id'),
            insert_ts=insert_ts,
            model_centile=row.get('model_centile'),
            model_decile=row.get('model_decile'),
            model_id=row.get('model_id'),
            model_score=round(model_score,9),
            model_subsegment=row.get('model_subsegment'),
            model_version=row.get('model_version'),
            round_cycle=row.get('round_cycle'),
            scoring_driver_1=row.get('scoring_driver_1'),
            scoring_driver_2=row.get('scoring_driver_2'),
            scoring_driver_3=row.get('scoring_driver_3'),
            insertion_timestamp=current_time 
        )
def run(args, beam_args):
    options = {
        'project': args.project,
        'runner': args.runner,
        'region': args.region,
        'staging_location': args.staging_location,
        'temp_location': args.temp_location,
        'template_location': args.template_location,
        'save_main_session': True,
        'streaming': False
    }

    pipeline_options = PipelineOptions.from_dictionary(options)
    template_options = pipeline_options.view_as(TemplateOptions)

    with beam.Pipeline(options=pipeline_options) as p:
        # Read from BigQuery
        data = (p | "ReadFromBigQuery" >> ReadFromBigQuery(
            query=template_options.query,
            use_standard_sql=True
        ))
        processed_data = (data | 'PrepareData' >> 
            beam.ParDo(PrepareData())
            .with_output_types(WlnModelScoresMutation))

        # Write to Spanner
        _ = (processed_data | "WriteToSpanner" >> 
            sp.SpannerInsertOrUpdate(
                project_id=args.spanner_project,
                instance_id=args.spanner_instance,
                database_id=args.spanner_dataset,
                table=args.spanner_table,
                expansion_service=args.expansion_service
            )
        )

def parse_arguments():
    parser = argparse.ArgumentParser()
    parser.add_argument('--project', required=True, help='Project ID')
    parser.add_argument('--runner', default="DataflowRunner", help='Pipeline runner')
    parser.add_argument('--region', default="us-east4", help='GCP project region')
    parser.add_argument('--staging_location', required=True, help='Staging GCS bucket path')
    parser.add_argument('--temp_location', required=True, help='Temp GCS bucket path')
    parser.add_argument('--template_location', required=True, help='GCS bucket path to save template')
    parser.add_argument('--spanner_dataset', required=True, help='spanner dataset')
    parser.add_argument('--spanner_instance', required=True, help='spanner instance')
    parser.add_argument('--spanner_project', required=True, help='spanner project')
    parser.add_argument('--spanner_table', required=True, help='spanner table')
    parser.add_argument('--expansion_service', default='localhost:50768', help='Expansion service host:port')
    return parser.parse_known_args()

if __name__ == '__main__':
    logging.getLogger().setLevel(logging.INFO)
    known_args, beam_args = parse_arguments()
    run(known_args, beam_args)
    print(f'Created template at {known_args.template_location}.')
