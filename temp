import apache_beam as beam
from apache_beam.options.pipeline_options import PipelineOptions
from apache_beam.io.gcp.pubsub import ReadFromPubSub
import apache_beam.io.gcp.spanner as sp
import json
import argparse
import logging
from typing import NamedTuple, Iterator, Optional
from datetime import datetime
from apache_beam.transforms.window import FixedWindows
from apache_beam.transforms.trigger import AfterWatermark, AccumulationMode

# Added imports for generating random strings and encoding avro records
import random
import string
import avro.schema
import avro.io
from io import BytesIO
import time
from avro.data import DatumWriter, DatumReader

class TemplateOptions(PipelineOptions):
    @classmethod
    def _add_argparse_args(cls, parser):
        parser.add_value_provider_argument('--params', type=str)

class InventoryCustomerProfilesMutation(NamedTuple):
    provisioning_date: Optional[str]
    data_circuit_id: Optional[str]
    circuit_id: Optional[str]
    video_circuit_id: Optional[str]
    service_type: Optional[str]
    address_id: Optional[int]
    vision_account_id: Optional[str]
    vision_customer_id: Optional[str]
    address_type: Optional[str]
    line_of_business: Optional[str]

beam.coders.registry.register_coder(InventoryCustomerProfilesMutation, beam.coders.RowCoder)

class DecodeAvroRecords(beam.DoFn):
    def process(self, element) -> Iterator[dict]:
        import avro.io as avro_io
        import avro.schema
        from io import BytesIO
        import time

        def reformat_input_msg_schema(msg):
            fmt_msg = {}
            fmt_msg['timestamp'] = msg['timestamp']
            fmt_msg['host'] = msg['host']
            fmt_msg['src'] = msg['src']
            fmt_msg['ingressTimestamp'] = msg['_event_ingress_ts']
            fmt_msg['origins'] = [msg['_event_origin']]
            tags_len = len(msg['_event_tags'])
            if tags_len > 0:
                if msg['_event_tags'][0] != "" and msg['_event_tags'][0] is not None:
                    fmt_msg['tags'] = msg['_event_tags']
                else:
                    fmt_msg['tags'] = ['Dummy']
            else:
                fmt_msg['tags'] = ['Dummy']
            fmt_msg['route'] = 3
            fmt_msg['fetchTimestamp'] = int(time.time() * 1000)
            fmt_msg['rawdata'] = msg['rawdata']

            return json.loads(fmt_msg['rawdata'])

        message: dict = {}
        raw_schema = """{"namespace": "com.vz.vznet",
                        "type": "record",
                        "name": "VznetDefault",
                        "doc": "Default schema for events in transit",
                        "fields": [
                        {"name": "timestamp", "type": "long"},
                        {"name": "host", "type": "string"},
                        {"name": "src",  "type": "string" },
                        {"name": "_event_ingress_ts", "type": "long"},
                        {"name": "_event_origin", "type": "string"},
                        {"name": "_event_tags", "type": {"type": "array", "items": "string"}},
                        {"name": "_event_route", "type": "string"},
                        {"name": "_event_metrics", "type": ["null", "bytes"], "default": null},
                        {"name": "rawdata", "type": "bytes"}
                        ]}"""
        try:
            schema = avro.schema.parse(raw_schema)
            avro_reader = avro_io.DatumReader(schema)
            avro_message = avro_io.BinaryDecoder(BytesIO(element))
            message = avro_reader.read(avro_message)

            if schema.name == 'VznetDefault':
                message['rawdata'] = message['rawdata'].decode("utf-8")
                message['_event_metrics'] = message['_event_metrics'].decode("utf-8") if message['_event_metrics'] is not None else None

            reformatted = reformat_input_msg_schema(message)
            yield reformatted

        except Exception as e:
            logging.error(f"Error processing message: {str(e)}")
            return

class PrepareSpannerData(beam.DoFn):
    def process(self, element) -> Iterator[InventoryCustomerProfilesMutation]:
        try:
            yield self.transform_to_mutation(element)
        except Exception as e:
            logging.error(f"Error processing element: {element}")
            logging.error(f"Error details: {str(e)}")
            return

    def format_timestamp(self,value):
        try:
            raw_value = float(value)
            if raw_value <= 0:
                raw_value = 0
            elif raw_value > 2240611199 * 1000:
                raw_value = 2240611199 * 1000 # date=2040-12-31

            casted_t_value = datetime.fromtimestamp(raw_value / 1000)
            return casted_t_value.strftime('%Y-%m-%dT%H:%M:%S.%fZ')
        except Exception as e:
            logging.error(f"CUSTOM_ERROR_LOG: Timestamp formatting error: {str(e)}; /n value={value}")
            return None

    def format_date(self, value):
        """Convert date/datetime to string format."""
        if value is None:
            return None
        try:
            if isinstance(value, str):
                # Remove microseconds if present
                dt = datetime.strptime(value.split('.')[0], '%Y-%m-%d %H:%M:%S')
                return dt.strftime('%Y-%m-%dT%H:%M:%S.%fZ')
            return value
        except Exception as e:
            logging.error(f"CUSTOM_ERROR_LOG: Date formatting error: {str(e)}")
            return None

    def safe_int(self, value):
        """Safely convert value to int."""
        if value is None:
            return None
        try:
            return int(value)
        except (ValueError, TypeError):
            return None

    def transform_to_mutation(self, element):
        return InventoryCustomerProfilesMutation(
            provisioning_date=self.format_timestamp(element.get('ont_activation_date')),
            data_circuit_id=element.get('data_circuit_id'),
            circuit_id=element.get('circuit_id'),
            video_circuit_id=element.get('video_circuit_id'),
            service_type=element.get('service_type'),
            address_id=self.safe_int(element.get('address_id')),
            vision_account_id=element.get('vision_account_id'),
            vision_customer_id=element.get('vision_customer_id'),
            address_type=element.get('address_type'),
            line_of_business=element.get('line_of_business')
        )

class BatchElements(beam.DoFn):
    def process(self, elements):
        yield elements

def run(known_args, beam_args):
    options = {
        'project': known_args.project,
        'runner': known_args.runner,
        'region': known_args.region,
        'staging_location': known_args.staging_location,
        'temp_location': known_args.temp_location,
        'template_location': known_args.template_location,
        'save_main_session': True,
        'streaming': True,
        'sdk_container_image': known_args.sdk_container_image,
        'sdk_location': 'container'
    }

    pipeline_options = PipelineOptions.from_dictionary(options)

    with beam.Pipeline(options=pipeline_options) as p:
        # Read and decode data
        data = (p
                | "Read From Pubsub" >> ReadFromPubSub(
                    subscription=f"{known_args.pubsub_subscription_name}")
                | "Decode Avro" >> beam.ParDo(DecodeAvroRecords())
                )

        # Process and batch data
        mutations = (data
                     | 'Prepare Spanner Data' >> beam.ParDo(PrepareSpannerData())
                     # Add timestamp to enable windowing
                     | 'Add Timestamps' >> beam.Map(lambda x: beam.window.TimestampedValue(x, beam.utils.timestamp.Timestamp.now()))
                     | 'Window' >> beam.WindowInto(
                         FixedWindows(120),  # 2 minutes in seconds
                         trigger=AfterWatermark(),
                         accumulation_mode=AccumulationMode.DISCARDING
                     )
                     | 'Global Window' >> beam.GroupByKey()
                     | 'Flatten Groups' >> beam.FlatMap(lambda x: x[1])
                     )

        # Write to Spanner
        _ = (mutations
             | "Write To Spanner" >> sp.SpannerInsertOrUpdate(
                 instance_id=known_args.spanner_instance,
                 database_id=known_args.spanner_dataset,
                 project_id=known_args.spanner_project,
                 table=known_args.spanner_table,
                 expansion_service=known_args.expansion_service,
                 commit_deadline=30,
                 max_number_mutations=7000,
                 high_priority=True
             )
             )

def parse_arguments():
    parser = argparse.ArgumentParser()
    parser.add_argument('--project', required=True, help='GCP Project ID')
    parser.add_argument('--runner', default="DataflowRunner", help='Pipeline runner')
    parser.add_argument('--region', default="us-east4", help='GCP project region')
    parser.add_argument('--staging_location', required=True, help='Staging GCS bucket path')
    parser.add_argument('--temp_location', required=True, help='Temp GCS bucket path')
    parser.add_argument('--template_location', required=True, help='GCS bucket path to save template')
    parser.add_argument('--pubsub_subscription_name', required=True, help='Input Subscription')
    parser.add_argument('--sdk_container_image',
                        default='us-east4-docker.pkg.dev/vz-it-np-gudv-dev-vzntdo-0/vznet/wireline:1.0.0',
                        help='sdk_container_image location')
    parser.add_argument('--spanner_dataset', required=True, help='spanner dataset')
    parser.add_argument('--spanner_instance', required=True, help='spanner instance')
    parser.add_argument('--spanner_project', required=True, help='spanner project')
    parser.add_argument('--spanner_table', required=True, help='spanner table')
    parser.add_argument('--expansion_service', default='localhost:50768', help='Expansion service host:port')
    return parser.parse_known_args()

if __name__ == '__main__':
    logging.getLogger().setLevel(logging.INFO)
    known_args, beam_args = parse_arguments()
    run(known_args, beam_args)
    print(f'Created template at {known_args.template_location}.')

# Helper functions for pushing messages to Pub/Sub
def generate_random_string(length):
    letters = string.ascii_lowercase
    return ''.join(random.choice(letters) for i in range(length))

def EncodeAvro(msg):
    raw_schema = """{"namespace": "com.vz.vznet",
                    "type": "record",
                    "name": "VznetDefault",
                    "doc": "Default schema for events in transit",
                    "fields": [
                    {"name": "timestamp", "type": "long"},
                    {"name": "host", "type": "string"},
                    {"name": "src",  "type": "string" },
                    {"name": "_event_ingress_ts", "type": "long"},
                    {"name": "_event_origin", "type": "string"},
                    {"name": "_event_tags", "type": {"type": "array", "items": "string"}},
                    {"name": "_event_route", "type": "string"},
                    {"name": "_event_metrics", "type": ["null", "bytes"], "default": null},
                    {"name": "rawdata", "type": "bytes"}
                    ]}"""

    parsed_schema = avro.schema.parse(raw_schema)

    # Prepare the message
    message = {}
    message['timestamp'] = msg['timestamp']
    message['host'] = msg['host']
    message['src'] = msg['src']
    message['_event_ingress_ts'] = msg['ingressTimestamp']
    message['_event_origin'] = ','.join(msg['origins'])  # Convert list to comma-separated string
    message['_event_tags'] = msg['tags']
    message['_event_route'] = str(msg['route'])
    message['_event_metrics'] = None
    message['rawdata'] = json.dumps(msg['rawdata']).encode('utf-8')

    # Encode the message
    bytes_writer = BytesIO()
    encoder = avro.io.BinaryEncoder(bytes_writer)
    writer = avro.io.DatumWriter(parsed_schema)
    writer.write(message, encoder)
    encoded_value = bytes_writer.getvalue()

    return encoded_value

def pushToTopic(data, topic_name):
    import google.cloud.pubsub_v1 as pubsub_v1
    publisher = pubsub_v1.PublisherClient()
    topic_path = publisher.topic_path("vz-it-np-j2lv-dev-cmido-0", topic_name) # Replace with your project ID and topic name
    data = data
    # Data must be a bytestring
    data = data
    # When you publish a message, the client returns a future.
    future = publisher.publish(topic_path, data=data)
    print(future.result())
    print(f"Published messages to {topic_path}.")

# Test data and Pub/Sub publishing
while True:
    pipTopologySrc = {
        "input": [
            {"src": "vz.pip.eclipse.stat.if_stats.proc.v0",
             "timestamp": int(time.time() * 1000),  # Current timestamp in milliseconds
             "host": "TEST-1234-XYZ",
             "ingressTimestamp": int(time.time() * 1000),
             "fetchTimestamp": int(time.time() * 1000),
             "origins": ["vmb", "kafka", "ENMV.PIP.IP"],
             "tags": [],
             "route": 3,
             "rawdata": {
                 "ont_activation_date": str(datetime.utcnow()), #Use a valid string type
                 "data_circuit_id": "A6o6LHciNWfMMDXRAk/AmzSqPzZU3lHpzvpeWb0487+2CKWc6iSMbMf+z7sLju0UiEj/jLnGDmcLlU5t5llkug==",
                 "circuit_id": f'{generate_random_string(20)}',
                 "video_circuit_id": f'{generate_random_string(20)}',
                 "service_type": "BWLHiXyrs2l8/F7ldbmoRsIY7WQhFyX3euXshykBAALkRUlgQLvnnltzAp/ymwFAjc0Auk7LVFC4InQT+g5/1Q==",
                 "address_id": random.randint(1000000, 9999999),  # Generate random integer
                 "vision_account_id": "ZcXG3TS/LG7ymRvKTaf4C7lUgLMO3MVHIhhQ74DRZ9Y6opYN1hADfRhn4SA+eXs5kbweoYtXXeAVLyo8XD1ioQ==",
                 "vision_customer_id": "Tz1bRQ1Md58vddT9EfHmU3LvXtmlDB4CDpLWNFRmI7wg4gFO9lSIEbNXNQAJiVqZIU2/dcdEjaZLlGHhFD4H0g==",
                 "address_type": "SFU",
                 "line_of_business": "Residence"
             }
             },
            {"src": "vz.pip.eclipse.stat.if_stats.proc.v0",
             "timestamp": int(time.time() * 1000),
             "host": "TEST-1234-XYZ",
             "ingressTimestamp": int(time.time() * 1000),
             "fetchTimestamp": int(time.time() * 1000),
             "origins": ["vmb", "kafka", "ENMV.PIP.IP"],
             "tags": [],
             "route": 3,
             "rawdata": {
                 "ont_activation_date": str(datetime.utcnow()),
                 "data_circuit_id": "A6o6LHciNWfMMDXRAk/AmzSqPzZU3lHpzvpeWb0487+2CKWc6iSMbMf+z7sLju0UiEj/jLnGDmcLlU5t5llkug==",
                 "circuit_id": f'{generate_random_string(20)}',
                 "video_circuit_id": f'{generate_random_string(20)}',
                 "service_type": "BWLHiXyrs2l8/F7ldbmoRsIY7WQhFyX3euXshykBAALkRUlgQLvnnltzAp/ymwFAjc0Auk7LVFC4InQT+g5/1Q==",
                 "address_id": random.randint(1000000, 9999999),
                 "vision_account_id": "ZcXG3TS/LG7ymRvKTaf4C7lUgLMO3MVHIhhQ74DRZ9Y6opYN1hADfRhn4SA+eXs5kbweoYtXXeAVLyo8XD1ioQ==",
                 "vision_customer_id": "Tz1bRQ1Md58vddT9EfHmU3LvXtmlDB4CDpLWNFRmI7wg4gFO9lSIEbNXNQAJiVqZIU2/dcdEjaZLlGHhFD4H0g==",
                 "address_type": "SFU",
                 "line_of_business": "Residence"
             }
             }
        ]
    }
    for i in pipTopologySrc['input']:
        e = EncodeAvro(i)
        pushToTopic(e, "wireline_churn_test_topic")
        time.sleep(5)
