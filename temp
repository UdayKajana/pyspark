import argparse
import logging
import time
from typing import NamedTuple, Iterator, Optional
from datetime import datetime, date
from decimal import Decimal

import apache_beam as beam
import apache_beam.io.gcp.spanner as sp
from apache_beam.options.pipeline_options import PipelineOptions
from apache_beam.io.gcp.bigquery import ReadFromBigQuery
from apache_beam.metrics import Metrics
from apache_beam.metrics.metric import MetricsFilter

class TemplateOptions(PipelineOptions):
    @classmethod
    def _add_argparse_args(cls, parser):
        parser.add_value_provider_argument('--query', type=str)

class WlnModelScoresMutation(NamedTuple):
    acct_sk: Optional[int]
    acct_type_cd: Optional[str]
    create_date: Optional[str]
    epsilon_cust_id: Optional[int]
    insert_ts: Optional[str]
    model_centile: Optional[int]
    model_decile: Optional[int]
    model_id: Optional[str]
    model_score: Optional[Decimal]
    model_subsegment: Optional[str]
    model_version: Optional[int]
    round_cycle: Optional[int]
    scoring_driver_1: Optional[str]
    scoring_driver_2: Optional[str]
    scoring_driver_3: Optional[str]
    insertion_timestamp: Optional[str]

beam.coders.registry.register_coder(WlnModelScoresMutation, beam.coders.RowCoder)

class MetricsMonitor:
    def __init__(self):
        # Processing Volume Metrics
        self.records_read = Metrics.counter('ProcessingMetrics', 'records_read')
        self.records_processed = Metrics.counter('ProcessingMetrics', 'records_processed')
        self.records_written = Metrics.counter('ProcessingMetrics', 'records_written')
        self.records_failed = Metrics.counter('ProcessingMetrics', 'records_failed')

        # Latency Metrics
        self.total_processing_time = Metrics.distribution('LatencyMetrics', 'total_processing_time_ms')
        self.transform_step_time = Metrics.distribution('LatencyMetrics', 'transform_step_time_ms')
        self.bigquery_read_latency = Metrics.distribution('LatencyMetrics', 'bigquery_read_latency_ms')
        self.spanner_write_latency = Metrics.distribution('LatencyMetrics', 'spanner_write_latency_ms')

        # Error Metrics
        self.validation_errors = Metrics.counter('ErrorMetrics', 'validation_errors')
        self.transform_errors = Metrics.counter('ErrorMetrics', 'transform_errors')
        self.write_errors = Metrics.counter('ErrorMetrics', 'write_errors')
        self.retry_attempts = Metrics.counter('ErrorMetrics', 'retry_attempts')
        self.dead_letter_count = Metrics.counter('ErrorMetrics', 'dead_letter_count')

class PrepareData(beam.DoFn):
    def __init__(self):
        super().__init__()
        self.metrics = MetricsMonitor()
        self.start_time = None
        self.batch_size = 0
        self.last_batch_time = None

    def setup(self):
        self.start_time = time.time()
        self.last_batch_time = self.start_time

    def process(self, element) -> Iterator[WlnModelScoresMutation]:
        process_start = time.time()

        try:
            # Record processing metrics
            self.metrics.records_read.inc()

            # Transform data
            transform_start = time.time()
            result = self.transform_to_mutation(element)
            transform_time = (time.time() - transform_start) * 1000
            self.metrics.transform_step_time.update(transform_time)

            # Record success
            self.metrics.records_processed.inc()
            yield result

        except ValueError as ve:
            self.metrics.validation_errors.inc()
            self.metrics.records_failed.inc()
            logging.error(f"Validation error: {str(ve)}")

        except Exception as e:
            self.metrics.transform_errors.inc()
            self.metrics.records_failed.inc()
            logging.error(f"Error processing element: {element}")
            logging.error(f"Error details: {str(e)}")
            self.metrics.retry_attempts.inc()
            raise
        finally:
            process_time = (time.time() - process_start) * 1000
            self.metrics.total_processing_time.update(process_time)

    def format_date(self, value):
        if value is None:
            return None
        if isinstance(value, (date, datetime)):
            return value.strftime('%Y-%m-%d')
        return value

    def format_timestamp(self, value):
        if value is None:
            return None
        if isinstance(value, datetime):
            return value.strftime('%Y-%m-%dT%H:%M:%S.%fZ')
        if isinstance(value, date):
            return f"{value.strftime('%Y-%m-%d')}T00:00:00.000000Z"
        return value

    def transform_to_mutation(self, row):
        transform_start = time.time()
        try:
            # Validate required fields
            if row.get('acct_sk') is None:
                raise ValueError("acct_sk is required")

            create_date = self.format_date(row.get('create_date'))
            insert_ts = self.format_timestamp(row.get('insert_ts'))
            model_score = row.get('model_score')
            if model_score is not None:
                model_score = Decimal(str(model_score)).quantize(Decimal('1e-9'))
            current_time = datetime.utcnow().strftime('%Y-%m-%dT%H:%M:%S.%fZ')

            return WlnModelScoresMutation(
                acct_sk=row.get('acct_sk'),
                acct_type_cd=row.get('acct_type_cd'),
                create_date=create_date,
                epsilon_cust_id=row.get('epsilon_cust_id'),
                insert_ts=insert_ts,
                model_centile=row.get('model_centile'),
                model_decile=row.get('model_decile'),
                model_id=row.get('model_id'),
                model_score=round(model_score,9) if model_score is not None else None,
                model_subsegment=row.get('model_subsegment'),
                model_version=row.get('model_version'),
                round_cycle=row.get('round_cycle'),
                scoring_driver_1=row.get('scoring_driver_1'),
                scoring_driver_2=row.get('scoring_driver_2'),
                scoring_driver_3=row.get('scoring_driver_3'),
                insertion_timestamp=current_time
            )
        finally:
            transform_time = (time.time() - transform_start) * 1000
            self.metrics.transform_step_time.update(transform_time)

class SpannerWriteMonitor(beam.DoFn):
    def __init__(self):
        super().__init__()
        self.metrics = MetricsMonitor()

    def process(self, element):
        write_start = time.time()
        try:
            yield element
            self.metrics.records_written.inc()
        except Exception as e:
            self.metrics.write_errors.inc()
            self.metrics.dead_letter_count.inc()
            raise
        finally:
            write_time = (time.time() - write_start) * 1000
            self.metrics.spanner_write_latency.update(write_time)

class BigQueryReadMonitor(beam.DoFn):
    def __init__(self):
        super().__init__()
        self.metrics = MetricsMonitor()
        self.read_start_time = None

    def setup(self):
        self.read_start_time = time.time()

    def process(self, element):
        yield element

    def teardown(self):
        read_end_time = time.time()
        read_latency = (read_end_time - self.read_start_time) * 1000
        self.metrics.bigquery_read_latency.update(read_latency)

def run(args, beam_args):
    job_start_time = time.time()
    options = {
        'project': args.project,
        'runner': args.runner,
        'region': args.region,
        'staging_location': args.staging_location,
        'temp_location': args.temp_location,
        'template_location': args.template_location,
        'save_main_session': True,
        'streaming': False
    }

    pipeline_options = PipelineOptions.from_dictionary(options)
    template_options = pipeline_options.view_as(TemplateOptions)

    with beam.Pipeline(options=pipeline_options) as p:
        # Read from BigQuery with latency monitoring
        read_start = time.time()
        data = (p 
            | "ReadFromBigQuery" >> ReadFromBigQuery(
            query=template_options.query,
            use_standard_sql=True
        )                
        | "MonitorBigQueryRead" >> beam.ParDo(BigQueryReadMonitor()))
        processed_data = (data
            | 'PrepareData' >> beam.ParDo(PrepareData()).with_output_types(WlnModelScoresMutation)
            | 'MonitorSpannerWrites' >> beam.ParDo(SpannerWriteMonitor()))

        # Write to Spanner
        _ = (processed_data | "WriteToSpanner" >>
            sp.SpannerInsertOrUpdate(
                project_id=args.spanner_project,
                instance_id=args.spanner_instance,
                database_id=args.spanner_dataset,
                table=args.spanner_table,
                expansion_service=args.expansion_service
            ))

def parse_arguments():
    parser = argparse.ArgumentParser()
    parser.add_argument('--project', required=True, help='Project ID')
    parser.add_argument('--runner', default="DataflowRunner", help='Pipeline runner')
    parser.add_argument('--region', default="us-east4", help='GCP project region')
    parser.add_argument('--staging_location', required=True, help='Staging GCS bucket path')
    parser.add_argument('--temp_location', required=True, help='Temp GCS bucket path')
    parser.add_argument('--template_location', required=True, help='GCS bucket path to save template')
    parser.add_argument('--spanner_dataset', required=True, help='spanner dataset')
    parser.add_argument('--spanner_instance', required=True, help='spanner instance')
    parser.add_argument('--spanner_project', required=True, help='spanner project')
    parser.add_argument('--spanner_table', required=True, help='spanner table')
    parser.add_argument('--expansion_service', default='localhost:50768', help='Expansion service host:port')
    return parser.parse_known_args()

if __name__ == '__main__':
    logging.getLogger().setLevel(logging.INFO)
    known_args, beam_args = parse_arguments()
    run(known_args, beam_args)
    print(f'Created template at {known_args.template_location}.')


Error message from worker: SDK harness sdk-0-0 disconnected. This usually means that the process running the pipeline code has crashed. Inspect the Worker Logs and the Diagnostics tab to determine the cause of the crash.
