Using the default container image
Waiting for container log creation
PYSPARK_PYTHON=/opt/dataproc/conda/bin/python
JAVA_HOME=/usr/lib/jvm/temurin-17-jdk-amd64
SPARK_EXTRA_CLASSPATH=
:: loading settings :: file = /etc/spark/conf/ivysettings.xml
/opt/dataproc/conda/bin/python: can't find '__main__' module in '/var/dataproc/tmp/srvls-batch-eaf0c2a0-7036-4c84-8245-c93ba2d33017/dataproc_job.py/'


import json
import requests
import time
import os
from pathlib import Path
import pandas
import geopandas as gp
import glob
import pandas as pd
import zipfile

def getRawCoverageMetadata(input_file, output_file):
    with open(input_file, 'r') as file:
        data = json.load(file)
    filtered_data = [item for item in data['data'] if item['data_type'] == 'Mobile Broadband Raw Coverage']
    with open(output_file, 'w') as file:
        json.dump({"data": filtered_data}, file, indent=4)
    
def downloadZipFiles(file_path, zipfiles_directory, headers):
    filecount = 1
    with open(file_path, 'r') as file:
        data = json.load(file)
        for entry in data['data']:
            # Removed continue statement to process all files
            # if filecount < 124:
            #    filecount = filecount + 1
            #    continue
            entry_id = entry['id']
            file_name = entry['file_name'] + '.zip'
            url = f"https://broadbandmap.fcc.gov/nbm/map/api/getNBMDataDownloadFile/{entry_id}/2"
            print(url)
            print(file_name)
            
            response = requests.get(url, headers=headers)
            response.raise_for_status()  # Raise HTTPError for bad responses (4xx or 5xx)
            with open(os.path.join(zipfiles_directory, file_name), 'wb') as output_file:
                output_file.write(response.content)
            time.sleep(45)
            print(filecount)
            filecount = filecount + 1

def unzipFiles(directory, output_directory):
    allFiles = os.listdir(directory)
    filtered = filter(lambda singleFile: singleFile.startswith('bdc_'), allFiles)
    filtered = list(filtered)
    for file in filtered:
        if file.endswith('.zip'):
            try:
                with zipfile.ZipFile(os.path.join(directory, file), 'r') as zip_ref:
                    zip_ref.extractall(output_directory)
            except zipfile.BadZipFile:
                print(f"Skipping bad zip file: {file}")

def convertToCsv(input_directory, output_directory):
    allFiles = os.listdir(input_directory)
    filtered = filter(lambda singleFile: singleFile.startswith('bdc_'), allFiles)
    filteredFiles = list(filtered)
    for file in filteredFiles:
        try:
            gdf = gp.read_file(os.path.join(input_directory, file))
            csv_filename = file[:file.index('.')] + '.csv'
            gdf.to_csv(os.path.join(output_directory, csv_filename), index=False)
        except Exception as e:
            print(f"Error converting {file} to CSV: {e}")

from google.cloud import storage

def download_blob(bucket_name, source_blob_name, destination_file_name):
    storage_client = storage.Client()
    bucket = storage_client.bucket(bucket_name)
    blob = bucket.blob(source_blob_name)
    blob.download_to_filename(destination_file_name)
    print(
        "Downloaded storage object {} from bucket {} to local file {}.".format(
            source_blob_name, bucket_name, destination_file_name
        )
    )

from google.cloud import bigquery
import pandas as pd
def upload_toBQ(file):
    try:
        df = pd.read_csv(file,
                          dtype={'frn': str, 'providerid': str, 'mindown': str,
                                 'minup': str, 'minsignal': str, 'environmnt': str,
                                 'brandname': str, 'technology': str, 'geometry': str})
        client = bigquery.Client()
        dataset_ref = client.dataset('market_intelligence_processed')
        table_ref = dataset_ref.table('bdc_test_data1')
        job = client.load_table_from_dataframe(df, table_ref)
        job.result()  # Wait for the job to complete

        print(f"Done with {file}")
    except Exception as e:
        print(f"Error uploading {file} to BigQuery: {e}")


import os
def list_files(path):
    for filename in os.listdir(path):
        file_path = os.path.join(path, filename)
        if os.path.isfile(file_path):
            yield file_path

def pushToBQ(folder_path):
    for file_path in list_files(folder_path):
        upload_toBQ(file_path)

if __name__ == '__main__':
    import json
    headers = {
        'accept': 'application/json, text/plain, */*',
        'accept-language': 'en-US,en;q=0.9',
        'cache-control': 'no-cache',
        'pragma': 'no-cache',
        'priority': 'u=1, i',
        'referer': 'https://broadbandmap.fcc.gov/data-download/data-by-provider?version=jun2024',
        'sec-ch-ua': '"Google Chrome";v="131", "Chromium";v="131", "Not_A Brand";v="24"',
        'sec-ch-ua-mobile': '?0',
        'sec-ch-ua-platform': '"macOS"',
        'sec-fetch-dest': 'empty',
        'sec-fetch-mode': 'cors',
        'sec-fetch-site': 'same-origin',
        'user-agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36'
        }
    import shutil
    try:
        shutil.rmtree('/tmp/bdc')
    except Exception as e:
        print("File Not found!")
    import os
    os.makedirs('/tmp/bdc/zipfiles', exist_ok=True)
    os.makedirs('/tmp/bdc/gpkgfiles', exist_ok=True)
    os.makedirs('/tmp/bdc/bdc_mobile_tmobile_processed', exist_ok=True)

    zipfiles_directory = '/tmp/bdc/zipfiles'
    metadata_file = '/tmp/bdc/raw_data.json'
    raw_coverage_metadata_file = '/tmp/bdc/filtered_data.json'
    
    #Removed initial json, no need.
    #data = {}
    #with open('/tmp/bdc/raw_data.json', 'w') as f:
    #    json.dump(data, f)
    
    download_blob("us-east4-vz-it-dev-wdwg-aid-d64a7958-bucket","dags/vz-it-j2lv-cmido-0/bdc_coverage/raw_data.json","/tmp/bdc/raw_data.json")
    getRawCoverageMetadata(metadata_file, raw_coverage_metadata_file)
    downloadZipFiles(raw_coverage_metadata_file, zipfiles_directory, headers)
    gpkgfiles_directory = '/tmp/bdc/gpkgfiles'
    unzipFiles(zipfiles_directory, gpkgfiles_directory)
    csvfiles_directory = '/tmp/bdc/bdc_mobile_tmobile_processed'
    convertToCsv(gpkgfiles_directory, csvfiles_directory)
    pushToBQ(csvfiles_directory)
the error is Job failed with message [requests.exceptions.ConnectionError: HTTPSConnectionPool(host='broadbandmap.fcc.gov', port=443): Max retries exceeded with url: /nbm/map/api/getNBMDataDownloadFile/892184/2 (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7f0ae7e7bb10>: Failed to establish a new connection: [Errno 101] Network is unreachable'))]. Additional details can be found at: https://console.cloud.google.com/dataproc/batches/us-east4/bdc-coverage-20250217-122637?project=vz-it-np-j2lv-dev-cmido-0 gcloud dataproc batches wait 'bdc-coverage-20250217-122637' --region 'us-east4' --project 'vz-it-np-j2lv-dev-cmido-0' https://console.cloud.google.com/storage/browser/dataproc-staging-us-east4-878261929676-qkkbu0xx/google-cloud-dataproc-metainfo/9e790309-ee10-49a0-91ea-55f376285a23/jobs/srvls-batch-991f9d5a-cf45-45c1-914b-dbab5302f800/ gs://dataproc-staging-us-east4-878261929676-qkkbu0xx/google-cloud-dataproc-metainfo/9e790309-ee10-49a0-91ea-55f376285a23/jobs/srvls-batch-991f9d5a-cf45-45c1-914b-dbab5302f800/driveroutput.*
