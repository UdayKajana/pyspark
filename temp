import apache_beam as beam
from apache_beam.options.pipeline_options import PipelineOptions
from apache_beam.io.gcp.pubsub import ReadFromPubSub
import apache_beam.io.gcp.spanner as sp
import json
import argparse
import logging
from typing import NamedTuple, Iterator, Optional
from datetime import datetime, timezone
from apache_beam.transforms.trigger import AfterWatermark, AfterProcessingTime, AccumulationMode, AfterCount,  OrFinally
import time

class TemplateOptions(PipelineOptions):
    @classmethod
    def _add_argparse_args(cls, parser):
        parser.add_value_provider_argument('--params', type=str)

class ConvertToSpannerMutation(NamedTuple):
    telephone_number: Optional[str]
    trouble_report_number: Optional[str]
    address_id: Optional[str]
    chronic_flag: Optional[str]
    chronic_total: Optional[str]
    circuit_id: Optional[str]
    circuit_type: Optional[str]
    reported_ts: Optional[str]
    data_circuit_id: Optional[str]
    video_circuit_id: Optional[str]
    window_timestamp: Optional[str]
    insertion_timestamp: Optional[str]

beam.coders.registry.register_coder(ConvertToSpannerMutation, beam.coders.RowCoder)

# Remove AttachArrivalTimestamp class

class DecodeAvroRecords(beam.DoFn):
    def process(self, element, timestamp=beam.DoFn.TimestampParam) -> Iterator[dict]:
        import avro.io as avro_io
        import avro.schema
        from io import BytesIO
        import time

        def reformat_input_msg_schema(msg):
            fmt_msg = {}
            fmt_msg['timestamp'] = msg['timestamp']
            fmt_msg['host'] = msg['host']
            fmt_msg['src'] = msg['src']
            fmt_msg['ingressTimestamp'] = msg['_event_ingress_ts']
            fmt_msg['origins'] = [msg['_event_origin']]
            tags_len = len(msg['_event_tags'])
            if tags_len > 0:
                if msg['_event_tags'][0] != "" and msg['_event_tags'][0] is not None:
                    fmt_msg['tags'] = msg['_event_tags']
                else:
                    fmt_msg['tags'] = ['Dummy']
            else:
                fmt_msg['tags'] = ['Dummy']
            fmt_msg['route'] = 3
            fmt_msg['fetchTimestamp'] = int(time.time() * 1000)
            fmt_msg['rawdata'] = msg['rawdata']
            return json.loads(fmt_msg['rawdata'])

        raw_schema = """{"namespace": "com.vz.vznet",
                        "type": "record",
                        "name": "VznetDefault",
                        "doc": "Default schema for events in transit",
                        "fields": [
                        {"name": "timestamp", "type": "long"},
                        {"name": "host", "type": "string"},
                        {"name": "src",  "type": "string" },
                        {"name": "_event_ingress_ts", "type": "long"},
                        {"name": "_event_origin", "type": "string"},
                        {"name": "_event_tags", "type": {"type": "array", "items": "string"}},
                        {"name": "_event_route", "type": "string"},
                        {"name": "_event_metrics", "type": ["null", "bytes"], "default": null},
                        {"name": "rawdata", "type": "bytes"}
                        ]}"""
        try:
            # Extract the actual data from the element
            avro_data = element['data'] if isinstance(element, dict) and 'data' in element else element
            schema = avro.schema.parse(raw_schema)
            avro_reader = avro_io.DatumReader(schema)
            if isinstance(avro_data, bytes):
                avro_message = avro_io.BinaryDecoder(BytesIO(avro_data))
                message = avro_reader.read(avro_message)
                if schema.name == 'VznetDefault':
                    message['rawdata'] = message['rawdata'].decode("utf-8")
                    message['_event_metrics'] = message['_event_metrics'].decode("utf-8") if message['_event_metrics'] is not None else None
                # Reformat the message
                decoded_message = reformat_input_msg_schema(message)
                # Extract event timestamp from decoded_message (assuming it's named 'event_time')
                #  and convert to seconds if it's in milliseconds
                event_time_ms = decoded_message.get('timestamp')  # Assuming 'timestamp' contains event time in milliseconds
                if event_time_ms is not None:
                    event_time = float(event_time_ms) / 1000.0  # Convert to seconds
                else:
                    event_time = time.time()  # Use the current time if the event time is unavailable

                # Yield TimestampedValue
                yield beam.window.TimestampedValue(decoded_message, event_time)
        except Exception as e:
            logging.error(f"Error processing message: {str(e)}")
            logging.error(f"Element type: {type(element)}")
            return

class PrepareSpannerData(beam.DoFn):
    def process(self, element, window=beam.DoFn.WindowParam) -> Iterator[ConvertToSpannerMutation]:
        try:
            window_start_utc = window.start.to_utc_datetime()
            window_start_str = window_start_utc.strftime('%Y-%m-%dT%H:%M:%S.%fZ')
            mutation = self.transform_to_mutation(element)
            if mutation:
                yield mutation._replace(window_timestamp=window_start_str)
        except Exception as e:
            logging.error(f"Error processing element: {element}")
            logging.error(f"Error details: {str(e)}")
            return
    def format_date(self, value):
        if value is None:
            return None
        try:
            if isinstance(value, str):
                dt = datetime.strptime(value.split('.')[0], '%Y-%m-%d %H:%M:%S')
                return dt.strftime('%Y-%m-%dT%H:%M:%S.%fZ')
            return value
        except Exception as e:
            logging.error(f"Date formatting error: {str(e)}")
            return None

    def safe_str(self, value):
        if value is None:
            return None
        try:
            return str(value)
        except Exception:
            return None

    def transform_to_mutation(self, element: dict):
        if not isinstance(element, dict):
            logging.error(f"Expected dictionary, got {type(element)}")
            return None
        if 'rawdata' in element:
            try:
                if isinstance(element['rawdata'], str):
                    element = json.loads(element['rawdata'])
                elif isinstance(element['rawdata'], dict):
                    element = element['rawdata']
            except json.JSONDecodeError as e:
                logging.error(f"Error decoding rawdata: {str(e)}")
                return None

        current_time = datetime.utcnow().strftime('%Y-%m-%dT%H:%M:%S.%fZ')
        return ConvertToSpannerMutation(
            telephone_number=element.get('telephone_number'),
            trouble_report_number=element.get('trouble_report_num'),
            address_id=element.get('address_id'),
            chronic_flag=element.get('chronic_flag'),
            chronic_total=element.get('chronic_total'),
            circuit_id=element.get('line_id_trimmed'),
            circuit_type=element.get('port_associated_service'),
            reported_ts=self.format_date(element.get('date_opened')),
            data_circuit_id=element.get('data_circuit_id'),
            video_circuit_id=self.safe_str(element.get('video_circuit_id')),
            window_timestamp=None,
            insertion_timestamp=current_time
        )

def run(known_args, beam_args):
    options = {
        'project': known_args.project,
        'runner': known_args.runner,
        'region': known_args.region,
        'staging_location': known_args.staging_location,
        'temp_location': known_args.temp_location,
        'template_location': known_args.template_location,
        'save_main_session': True,
        'streaming': True,
        'sdk_container_image': known_args.sdk_container_image,
        'sdk_location': 'container'
    }

    pipeline_options = PipelineOptions.from_dictionary(options)

    with beam.Pipeline(options=pipeline_options) as p:
        data = (p
            | "Read From Pubsub" >> ReadFromPubSub(
                subscription=f"{known_args.pubsub_subscription_name}",
                with_attributes=True,
                timestamp_attribute=None
            )
            | "Decode Avro" >> beam.ParDo(DecodeAvroRecords())
            | "Window Into Fixed Intervals" >> beam.WindowInto(
    beam.window.FixedWindows(120),
    trigger=AfterWatermark(
        early=AfterProcessingTime(120) or AfterCount(100),
        late=AfterCount(1)
    ),
    allowed_lateness=beam.transforms.window.Duration(seconds=300),
    accumulation_mode=AccumulationMode.ACCUMULATING
)
            | 'Prepare Spanner Data' >> beam.ParDo(PrepareSpannerData())
            | 'Add Key' >> beam.Map(lambda x: (1, x)) 
            | 'Group Into Batches' >> beam.GroupByKey()
            | 'Ungroup' >> beam.FlatMap(lambda x: x[1])
        )

        _ = (data
            | "Write To Spanner" >> sp.SpannerInsertOrUpdate(
                instance_id=known_args.spanner_instance,
                database_id=known_args.spanner_dataset,
                project_id=known_args.spanner_project,
                table=known_args.spanner_table,
                expansion_service=known_args.expansion_service,
                max_number_mutations=7000,
                high_priority=True
            )
        )
def parse_arguments():
    parser = argparse.ArgumentParser()
    parser.add_argument('--project', required=True, help='GCP Project ID')
    parser.add_argument('--runner', default="DataflowRunner", help='Pipeline runner')
    parser.add_argument('--region', default="us-east4", help='GCP project region')
    parser.add_argument('--staging_location', required=True, help='Staging GCS bucket path')
    parser.add_argument('--temp_location', required=True, help='Temp GCS bucket path')
    parser.add_argument('--template_location', required=True, help='GCS bucket path to save template')
    parser.add_argument('--pubsub_subscription_name', required=True, help='Input Subscription')
    parser.add_argument('--sdk_container_image', default='us-east4-docker.pkg.dev/vz-it-np-gudv-dev-vzntdo-0/vznet/wireline:1.0.0', help='sdk_container_image location')
    parser.add_argument('--spanner_dataset', required=True, help='spanner dataset')
    parser.add_argument('--spanner_instance', required=True, help='spanner instance')
    parser.add_argument('--spanner_project', required=True, help='spanner project')
    parser.add_argument('--spanner_table', required=True, help='spanner table')
    parser.add_argument('--expansion_service', default='localhost:50768', help='Expansion service host:port')
    return parser.parse_known_args()

if __name__ == '__main__':
    logging.getLogger().setLevel(logging.INFO)
    known_args, beam_args = parse_arguments()
    run(known_args, beam_args)
    print(f'Created template at {known_args.template_location}.')
import json
import avro.io as avro_io
import avro.schema
from io import BytesIO
from avro.datafile import DataFileReader, DataFileWriter
import io
import logging
import argparse
from subprocess import getoutput
import random
def pushToTopic(data_rows, topic_name):
    from concurrent import futures
    from google.cloud import pubsub_v1
    from google.cloud.pubsub_v1.types import (
        LimitExceededBehavior,
        PublisherOptions,
        PublishFlowControl,
    )

    # TODO(developer)
    project_id = "vz-it-np-gudv-dev-vzntdo-0"
    topic_id = topic_name  # "dflow_test"

    # Configure how many messages the publisher client can hold in memory
    # and what to do when messages exceed the limit.
    publisher = pubsub_v1.PublisherClient()

    topic_path = publisher.topic_path(project_id, topic_id)

    publish_futures = []

    # Resolve the publish future in a separate thread.
    def callback(publish_future: pubsub_v1.publisher.futures.Future) -> None:
        message_id = publish_future.result()
        # print(message_id)
    publish_future = publisher.publish(topic_path, data_rows)
    print(publish_future)
    # future.result(timeout=10)
    # Non-blocking. Allow the publisher client to batch messages.
    publish_future.add_done_callback(callback)
    publish_futures.append(publish_future)
    print(f"Published messages with flow control settings to {topic_path}.")

def EncodeAvro(element):
    def encode(data: dict) -> bytes:
        schema = """{"namespace": "com.vz.vznet",
                                                               "type": "record",
                                                               "name": "VznetDefault",
                                                               "doc": "Default schema for events in transit",
                                                               "fields": [
                                                               {"name": "timestamp", "type": "long"},
                                                               {"name": "host", "type": "string"},
                                                               {"name": "src",  "type": "string" },
                                                               {"name": "_event_ingress_ts", "type": "long"},
                                                               {"name": "_event_origin", "type": "string"},
                                                               {"name": "_event_tags", "type": {"type": "array", "items": "string"}},
                                                               {"name": "_event_route", "type": "string"},
                                                               {"name": "_event_metrics", "type": ["null", "bytes"], "default": null},
                                                               {"name": "rawdata", "type": "bytes"}
                                                               ]
                                                               }"""
        if isinstance(schema, dict):
            schema = json.dumps(schema)
        if isinstance(data, str):
            data = json.loads(data)
        schema: str = avro.schema.parse(schema)
        val = data
        # dataFile = open(file, "wb")
        # val[DefaultValues.EVENT_METRICS] = bytes(str(val.get(DefaultValues.EVENT_METRICS)), 'utf-8')
        datum_writer: avro_io.DatumWriter = avro_io.DatumWriter(schema)
        bytes_writer: io.BytesIO = io.BytesIO()
        encoder: avro_io.BinaryEncoder = avro_io.BinaryEncoder(bytes_writer)
        datum_writer.write(val, encoder)
        raw_bytes: bytes = bytes_writer.getvalue()
        return raw_bytes

    raw_data = element['rawdata']
    final_dict = {}
    final_dict['timestamp'] = element['timestamp']
    final_dict['src'] = element['src']
    final_dict['host'] = element['host']
    final_dict['_event_ingress_ts'] = element['ingressTimestamp']
    final_dict['_event_origin'] = '|'.join(element['origins'])
    final_dict["rawdata"] = json.dumps(raw_data)
    final_dict['_event_tags'] = element['tags']
    final_dict['_event_route'] = str(element['route'])
    final_dict['_event_metrics'] = None
    final_dict["rawdata"] = bytes(json.dumps(raw_data), 'utf-8')
    return encode(final_dict)
from datetime import datetime
import random
import string

def generate_random_string(length):
  """Generates a random string of specified length."""
  characters = string.ascii_letters + string.digits
  return ''.join(random.choice(characters) for _ in range(length))

random_string = generate_random_string(20)
print(random_string)
pipTopologySrc = {
    "input": [
        {"src": "vz.pip.eclipse.stat.if_stats.proc.v0", 
         "timestamp": 1563152004139, 
         "host": "TEST-1234-XYZ",
         "ingressTimestamp": 1563153004139, 
         "fetchTimestamp": 1563153008139, 
         "origins": ["vmb,kafka,ENMV.PIP.IP"],
         "tags": [], "route": 3,
"rawdata": {
"telephone_number": "2022-12-02 05:00:00.000000 UTC",
"trouble_report_num" : f"{str(random.randint(0,99999))}",
"addres1s_id" : "123",  
"chronic_flag" : "LGF8pf4gkOZcABnNq9NMxNcE/U2Gc0B3trVK0hEzNp7v5Z7ONCJVcILwpbzVuqyOFxz2XIU/rjOerp6NEmjmnQ==",
"chronic1_total" : "BWLHiXyrs2l8/F7ldbmoRsIY7WQhFyX3euXshykBAALkRUlgQLvnnltzAp/ymwFAjc0Auk7LVFC4InQT+g5/1Q==",
"line_id_trimmed" : "20808267725.0",
"port_as1sociated_service" : "ZcXG3TS/LG7ymRvKTaf4C7lUgLMO3MVHIhhQ74DRZ9Y6opYN1hADfRhn4SA+eXs5kbweoYtXXeAVLyo8XD1ioQ==",
"date_opened" : "2019-08-28 04:00:00.000000 UTC",
"data_circuit_id" : "SFU",
"video_circuit_id" : "Residence"
}
#             "rawdata": {
#              "ont_activation_date": "2019-08-28 04:00:00.000000 UTC",
#              "data_circuit_id" : "A6o6LHciNWfMMDXRAk/AmzSqPzZU3lHpzvpeWb0487+2CKWc6iSMbMf+z7sLju0UiEj/jLnGDmcLlU5t5llkug==",
#              "circuit_id" : f'{generate_random_string(20)}',
#              "video_circuit_id" :  f'{generate_random_string(20)}',
#              "service_type" : "BWLHiXyrs2l8/F7ldbmoRsIY7WQhFyX3euXshykBAALkRUlgQLvnnltzAp/ymwFAjc0Auk7LVFC4InQT+g5/1Q==",
#              "address_id" : 17935568585.0,
#              "vision_account_id" : "ZcXG3TS/LG7ymRvKTaf4C7lUgLMO3MVHIhhQ74DRZ9Y6opYN1hADfRhn4SA+eXs5kbweoYtXXeAVLyo8XD1ioQ==",
#              "vision_customer_id" : "Tz1bRQ1Md58vddT9EfHmU3LvXtmlDB4CDpLWNFRmI7wg4gFO9lSIEbNXNQAJiVqZIU2/dcdEjaZLlGHhFD4H0g==",
#              "address_type11" : "SFU",
#              "line_of_business" : "Residence"
#          }
        },
        {"src": "vz.pip.eclipse.stat.if_stats.proc.v0", 
         "timestamp": 1563152004139, 
         "host": "TEST-1234-XYZ",
         "ingressTimestamp": 1563153004139, 
         "fetchTimestamp": 1563153008139, 
         "origins": ["vmb,kafka,ENMV.PIP.IP"],
         "tags": [], 
         "route": 3,
            "rawdata": {
            "telephone_number": "2022-12-02 05:00:00.000000 UTC",
            "trouble_report_num" : f"{str(random.randint(0,99999))}",
            "addres1s_id" : "134",
            "chronic_flag" : "LGF8pf4gkOZcABnNq9NMxNcE/U2Gc0B3trVK0hEzNp7v5Z7ONCJVcILwpbzVuqyOFxz2XIU/rjOerp6NEmjmnQ==",
            "chronic_1total" : "BWLHiXyrs2l8/F7ldbmoRsIY7WQhFyX3euXshykBAALkRUlgQLvnnltzAp/ymwFAjc0Auk7LVFC4InQT+g5/1Q==",
            "line_id_trimmed" : "20808267725.0",
            "port_as1sociated_service" : "ZcXG3TS/LG7ymRvKTaf4C7lUgLMO3MVHIhhQ74DRZ9Y6opYN1hADfRhn4SA+eXs5kbweoYtXXeAVLyo8XD1ioQ==",
            "date_opened" : "2019-08-28 04:00:00.000000 UTC",
            "data_circuit_id" : "SFU",
            "video_circuit_id" : "Residence"
            }
#         "rawdata": {
#              "ont_activation_date": "2019-08-28 04:00:00.000000 UTC",
#              "data_circuit_id" : "A6o6LHciNWfMMDXRAk/AmzSqPzZU3lHpzvpeWb0487+2CKWc6iSMbMf+z7sLju0UiEj/jLnGDmcLlU5t5llkug==",
#              "circuit_id" : f'{generate_random_string(20)}',
#              "video_circuit_id" : f'{generate_random_string(20)}',
#              "service_type11" : "BWLHiXyrs2l8/F7ldbmoRsIY7WQhFyX3euXshykBAALkRUlgQLvnnltzAp/ymwFAjc0Auk7LVFC4InQT+g5/1Q==",
#              "address_id" : 17935568585.0,
#              "vision_account_id" : "ZcXG3TS/LG7ymRvKTaf4C7lUgLMO3MVHIhhQ74DRZ9Y6opYN1hADfRhn4SA+eXs5kbweoYtXXeAVLyo8XD1ioQ==",
#              "vision_customer_id" : "Tz1bRQ1Md58vddT9EfHmU3LvXtmlDB4CDpLWNFRmI7wg4gFO9lSIEbNXNQAJiVqZIU2/dcdEjaZLlGHhFD4H0g==",
#              "address_type" : "SFU",
#              "line_of_business" : "Residence"
#          }
        }
        ]
    }
for i in pipTopologySrc['input']:
    e = EncodeAvro(i)
    pushToTopic(e, "vz.common.vrepair.ticket.opened.norm.v0")
#     pushToTopic(e, "dim.inventory.customer.profiles.norm_v0")
