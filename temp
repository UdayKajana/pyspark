import json
import requests
import time
import os
from pathlib import Path
import pandas
import geopandas as gp
import glob
import pandas as pd
import zipfile

def getRawCoverageMetadata(input_file, output_file):
    with open(input_file, 'r') as file:
        data = json.load(file)
    filtered_data = [item for item in data['data'] if item['data_type'] == 'Mobile Broadband Raw Coverage']
    with open(output_file, 'w') as file:
        json.dump({"data": filtered_data}, file, indent=4)
    
def downloadZipFiles(file_path, zipfiles_directory, headers):
    filecount = 1
    with open(file_path, 'r') as file:
        data = json.load(file)
        for entry in data['data']:
            if filecount < 124:
                filecount = filecount + 1
                continue
            entry_id = entry['id']
            file_name = entry['file_name'] + '.zip'
            url = f"https://broadbandmap.fcc.gov/nbm/map/api/getNBMDataDownloadFile/{entry_id}/2"
            print(url)
            print(file_name)
            
            response = requests.get(url, headers=headers)
            with open(os.path.join(zipfiles_directory, file_name), 'wb') as output_file:
                output_file.write(response.content)
            time.sleep(45)
            print(filecount)
            filecount = filecount + 1

def unzipFiles(directory, output_directory):
    allFiles = os.listdir(directory)
    filtered = filter(lambda singleFile: singleFile.startswith('bdc_'), allFiles)
    filtered = list(filtered)
    for file in filtered:
        if file.endswith('.zip'):
            with zipfile.ZipFile(os.path.join(directory, file), 'r') as zip_ref:
                zip_ref.extractall(output_directory)

def convertToCsv(input_directory, output_directory):
    allFiles = os.listdir(input_directory)
    filtered = filter(lambda singleFile: singleFile.startswith('bdc_'), allFiles)
    filteredFiles = list(filtered)
    for file in filteredFiles:
        gdf = gp.read_file(os.path.join(input_directory, file))
        csv_filename = file[:file.index('.')] + '.csv'
        gdf.to_csv(os.path.join(output_directory, csv_filename), index=False)
from google.cloud import storage

def download_blob(bucket_name, source_blob_name, destination_file_name):
    storage_client = storage.Client()
    bucket = storage_client.bucket(bucket_name)
    blob = bucket.blob(source_blob_name)
    blob.download_to_filename(destination_file_name)
    print(
        "Downloaded storage object {} from bucket {} to local file {}.".format(
            source_blob_name, bucket_name, destination_file_name
        )
    )

from google.cloud import bigquery
import pandas as pd
def upload_toBQ(file):
    df = pd.read_csv(file,
                      dtype={'frn': str, 'providerid': str, 'mindown': str, 
                             'minup': str, 'minsignal': str, 'environmnt': str,
                             'brandname': str, 'technology': str, 'geometry': str})  
    client = bigquery.Client()
    dataset_ref = client.dataset('market_intelligence_processed')
    table_ref = dataset_ref.table('bdc_test_data1')
    client.load_table_from_dataframe(df, table_ref).result()
    print(f"Done with {file}")
import os
def list_files(path):
    for filename in os.listdir(path):
        file_path = os.path.join(path, filename)
        if os.path.isfile(file_path):
            yield file_path

def pushToBQ(folder_path):
    for file_path in list_files(folder_path):
        upload_toBQ(file_path)

if __name__ == '__main__':
    import json
    headers = {
        'accept': 'application/json, text/plain, */*',
        'accept-language': 'en-US,en;q=0.9',
        'cache-control': 'no-cache',
        'pragma': 'no-cache',
        'priority': 'u=1, i',
        'referer': 'https://broadbandmap.fcc.gov/data-download/data-by-provider?version=jun2024',
        'sec-ch-ua': '"Google Chrome";v="131", "Chromium";v="131", "Not_A Brand";v="24"',
        'sec-ch-ua-mobile': '?0',
        'sec-ch-ua-platform': '"macOS"',
        'sec-fetch-dest': 'empty',
        'sec-fetch-mode': 'cors',
        'sec-fetch-site': 'same-origin',
        'user-agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36'
        }
    import shutil
    try:
        shutil.rmtree('/tmp/bdc')
    except Exception as e:
        print("File Not found!")
    import os
    os.mkdir('/tmp/bdc')
    os.mkdir('/tmp/bdc/zipfiles')
    os.mkdir('/tmp/bdc/gpkgfiles')
    os.mkdir('/tmp/bdc/bdc_mobile_tmobile_processed')
    zipfiles_directory = '/tmp/bdc/zipfiles'
    metadata_file = '/tmp/bdc/raw_data.json'
    raw_coverage_metadata_file = '/tmp/bdc/filtered_data.json'
    data = {}
    with open('/tmp/bdc/raw_data.json', 'w') as f:
        json.dump(data, f)
    download_blob("us-east4-vz-it-dev-wdwg-aid-d64a7958-bucket","dags/vz-it-j2lv-cmido-0/bdc_coverage/raw_data.json","/tmp/bdc/raw_data.json")
    getRawCoverageMetadata(metadata_file, raw_coverage_metadata_file)
    downloadZipFiles(raw_coverage_metadata_file, zipfiles_directory, headers)
    gpkgfiles_directory = '/tmp/bdc/gpkgfiles'
    unzipFiles(zipfiles_directory, gpkgfiles_directory)
    csvfiles_directory = '/tmp/bdc/bdc_mobile_tmobile_processed'
    convertToCsv(gpkgfiles_directory, csvfiles_directory)
    pushToBQ(csvfiles_directory)

the error is 

[2025-02-17, 11:47:19 UTC] {warnings.py:110} WARNING - /opt/python3.11/lib/python3.11/site-packages/airflow/providers/openlineage/plugins/adapter.py:93: DeprecationWarning: `OpenLineageClient.from_environment()` is deprecated. Use `OpenLineageClient()`.
  self._client = OpenLineageClient.from_environment()

[2025-02-17, 11:47:19 UTC] {client.py:110} INFO - OpenLineageClient will use `http` transport
[2025-02-17, 11:47:24 UTC] {adapter.py:163} WARNING - Failed to emit OpenLineage event of id 019513b5-0e69-7de8-9ecc-7037db460a5f
[2025-02-17, 11:47:24 UTC] {connection.py:232} WARNING - Connection schemes (type: google_cloud_platform) shall not contain '_' according to RFC3986.
[2025-02-17, 11:47:24 UTC] {base.py:73} INFO - Using connection ID '***' for task execution.
[2025-02-17, 11:47:24 UTC] {dataproc.py:2998} INFO - Creating batch bdc-coverage-20250217-114718
[2025-02-17, 11:47:24 UTC] {dataproc.py:2999} INFO - Once started, the batch job will be available at https://console.cloud.google.com/dataproc/batches/us-east4/bdc-coverage-20250217-114718/monitoring?project=vz-it-np-j2lv-dev-cmido-0
[2025-02-17, 11:53:00 UTC] {warnings.py:110} WARNING - /opt/python3.11/lib/python3.11/site-packages/airflow/providers/openlineage/utils/utils.py:541: AirflowProviderDeprecationWarning: Call to deprecated class UnknownOperatorAttributeRunFacet. (To be removed in the next release. Make sure to use information from AirflowRunFacet instead.)
  UnknownOperatorAttributeRunFacet(

[2025-02-17, 11:53:00 UTC] {warnings.py:110} WARNING - /opt/python3.11/lib/python3.11/site-packages/airflow/providers/openlineage/plugins/adapter.py:93: DeprecationWarning: `OpenLineageClient.from_environment()` is deprecated. Use `OpenLineageClient()`.
  self._client = OpenLineageClient.from_environment()

[2025-02-17, 11:53:00 UTC] {client.py:110} INFO - OpenLineageClient will use `http` transport
[2025-02-17, 11:53:05 UTC] {adapter.py:163} WARNING - Failed to emit OpenLineage event of id 019513b5-0e69-7de8-9ecc-7037db460a5f
[2025-02-17, 11:53:05 UTC] {taskinstance.py:1939} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/opt/python3.11/lib/python3.11/site-packages/airflow/providers/google/cloud/hooks/dataproc.py", line 267, in wait_for_operation
    return operation.result(timeout=timeout, retry=result_retry)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/python3.11/lib/python3.11/site-packages/google/api_core/future/polling.py", line 261, in result
    raise self._exception
google.api_core.exceptions.Aborted: 409 Google Cloud Dataproc Agent reports job failure. If logs are available, they can be found at:
https://console.cloud.google.com/dataproc/batches/us-east4/bdc-coverage-20250217-114718?project=vz-it-np-j2lv-dev-cmido-0
gcloud dataproc batches wait 'bdc-coverage-20250217-114718' --region 'us-east4' --project 'vz-it-np-j2lv-dev-cmido-0'
https://console.cloud.google.com/storage/browser/dataproc-staging-us-east4-878261929676-qkkbu0xx/google-cloud-dataproc-metainfo/cad32c6e-9ce0-42b0-b3db-ac4a7a934c1d/jobs/srvls-batch-eaf0c2a0-7036-4c84-8245-c93ba2d33017/
gs://dataproc-staging-us-east4-878261929676-qkkbu0xx/google-cloud-dataproc-metainfo/cad32c6e-9ce0-42b0-b3db-ac4a7a934c1d/jobs/srvls-batch-eaf0c2a0-7036-4c84-8245-c93ba2d33017/driveroutput.* 10: Google Cloud Dataproc Agent reports job failure. If logs are available, they can be found at:
https://console.cloud.google.com/dataproc/batches/us-east4/bdc-coverage-20250217-114718?project=vz-it-np-j2lv-dev-cmido-0
gcloud dataproc batches wait 'bdc-coverage-20250217-114718' --region 'us-east4' --project 'vz-it-np-j2lv-dev-cmido-0'
https://console.cloud.google.com/storage/browser/dataproc-staging-us-east4-878261929676-qkkbu0xx/google-cloud-dataproc-metainfo/cad32c6e-9ce0-42b0-b3db-ac4a7a934c1d/jobs/srvls-batch-eaf0c2a0-7036-4c84-8245-c93ba2d33017/
gs://dataproc-staging-us-east4-878261929676-qkkbu0xx/google-cloud-dataproc-metainfo/cad32c6e-9ce0-42b0-b3db-ac4a7a934c1d/jobs/srvls-batch-eaf0c2a0-7036-4c84-8245-c93ba2d33017/driveroutput.*

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/python3.11/lib/python3.11/site-packages/airflow/providers/google/cloud/operators/dataproc.py", line 3020, in execute
    result = hook.wait_for_operation(
             ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/python3.11/lib/python3.11/site-packages/airflow/providers/google/cloud/hooks/dataproc.py", line 270, in wait_for_operation
    raise AirflowException(error)
airflow.exceptions.AirflowException: 409 Google Cloud Dataproc Agent reports job failure. If logs are available, they can be found at:
https://console.cloud.google.com/dataproc/batches/us-east4/bdc-coverage-20250217-114718?project=vz-it-np-j2lv-dev-cmido-0
gcloud dataproc batches wait 'bdc-coverage-20250217-114718' --region 'us-east4' --project 'vz-it-np-j2lv-dev-cmido-0'
https://console.cloud.google.com/storage/browser/dataproc-staging-us-east4-878261929676-qkkbu0xx/google-cloud-dataproc-metainfo/cad32c6e-9ce0-42b0-b3db-ac4a7a934c1d/jobs/srvls-batch-eaf0c2a0-7036-4c84-8245-c93ba2d33017/
gs://dataproc-staging-us-east4-878261929676-qkkbu0xx/google-cloud-dataproc-metainfo/cad32c6e-9ce0-42b0-b3db-ac4a7a934c1d/jobs/srvls-batch-eaf0c2a0-7036-4c84-8245-c93ba2d33017/driveroutput.* 10: Google Cloud Dataproc Agent reports job failure. If logs are available, they can be found at:
https://console.cloud.google.com/dataproc/batches/us-east4/bdc-coverage-20250217-114718?project=vz-it-np-j2lv-dev-cmido-0
gcloud dataproc batches wait 'bdc-coverage-20250217-114718' --region 'us-east4' --project 'vz-it-np-j2lv-dev-cmido-0'
https://console.cloud.google.com/storage/browser/dataproc-staging-us-east4-878261929676-qkkbu0xx/google-cloud-dataproc-metainfo/cad32c6e-9ce0-42b0-b3db-ac4a7a934c1d/jobs/srvls-batch-eaf0c2a0-7036-4c84-8245-c93ba2d33017/
gs://dataproc-staging-us-east4-878261929676-qkkbu0xx/google-cloud-dataproc-metainfo/cad32c6e-9ce0-42b0-b3db-ac4a7a934c1d/jobs/srvls-batch-eaf0c2a0-7036-4c84-8245-c93ba2d33017/driveroutput.*
[2025-02-17, 11:53:05 UTC] {taskinstance.py:1401} INFO - Marking task as FAILED. dag_id=dg_bdc_raw_coverage, task_id=bdc_raw_coverage, execution_date=20250217T113947, start_date=20250217T114718, end_date=20250217T115305
[2025-02-17, 11:53:05 UTC] {warnings.py:110} WARNING - /opt/python3.11/lib/python3.11/site-packages/airflow/utils/email.py:154: RemovedInAirflow3Warning: Fetching SMTP credentials from configuration variables will be deprecated in a future release. Please set credentials using a connection instead.
  send_mime_email(e_from=mail_from, e_to=recipients, mime_msg=msg, conn_id=conn_id, dryrun=dryrun)

[2025-02-17, 11:53:05 UTC] {configuration.py:1067} WARNING - section/key [smtp/smtp_user] not found in config
[2025-02-17, 11:53:05 UTC] {email.py:270} INFO - Email alerting: attempt 1
[2025-02-17, 11:53:05 UTC] {email.py:281} INFO - Sent an alert email to ['vuyyuri.trilok.narayana.varma@verizon.com']
[2025-02-17, 11:53:05 UTC] {taskinstance.py:1707} ERROR - Error when executing partial callback
Traceback (most recent call last):
  File "/opt/python3.11/lib/python3.11/site-packages/airflow/providers/google/cloud/hooks/dataproc.py", line 267, in wait_for_operation
    return operation.result(timeout=timeout, retry=result_retry)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/python3.11/lib/python3.11/site-packages/google/api_core/future/polling.py", line 261, in result
    raise self._exception
google.api_core.exceptions.Aborted: 409 Google Cloud Dataproc Agent reports job failure. If logs are available, they can be found at:
https://console.cloud.google.com/dataproc/batches/us-east4/bdc-coverage-20250217-114718?project=vz-it-np-j2lv-dev-cmido-0
gcloud dataproc batches wait 'bdc-coverage-20250217-114718' --region 'us-east4' --project 'vz-it-np-j2lv-dev-cmido-0'
https://console.cloud.google.com/storage/browser/dataproc-staging-us-east4-878261929676-qkkbu0xx/google-cloud-dataproc-metainfo/cad32c6e-9ce0-42b0-b3db-ac4a7a934c1d/jobs/srvls-batch-eaf0c2a0-7036-4c84-8245-c93ba2d33017/
gs://dataproc-staging-us-east4-878261929676-qkkbu0xx/google-cloud-dataproc-metainfo/cad32c6e-9ce0-42b0-b3db-ac4a7a934c1d/jobs/srvls-batch-eaf0c2a0-7036-4c84-8245-c93ba2d33017/driveroutput.* 10: Google Cloud Dataproc Agent reports job failure. If logs are available, they can be found at:
https://console.cloud.google.com/dataproc/batches/us-east4/bdc-coverage-20250217-114718?project=vz-it-np-j2lv-dev-cmido-0
gcloud dataproc batches wait 'bdc-coverage-20250217-114718' --region 'us-east4' --project 'vz-it-np-j2lv-dev-cmido-0'
https://console.cloud.google.com/storage/browser/dataproc-staging-us-east4-878261929676-qkkbu0xx/google-cloud-dataproc-metainfo/cad32c6e-9ce0-42b0-b3db-ac4a7a934c1d/jobs/srvls-batch-eaf0c2a0-7036-4c84-8245-c93ba2d33017/
gs://dataproc-staging-us-east4-878261929676-qkkbu0xx/google-cloud-dataproc-metainfo/cad32c6e-9ce0-42b0-b3db-ac4a7a934c1d/jobs/srvls-batch-eaf0c2a0-7036-4c84-8245-c93ba2d33017/driveroutput.*

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/python3.11/lib/python3.11/site-packages/airflow/models/taskinstance.py", line 1519, in _run_raw_task
    self._execute_task_with_callbacks(context, test_mode, session=session)
  File "/opt/python3.11/lib/python3.11/site-packages/airflow/models/taskinstance.py", line 1683, in _execute_task_with_callbacks
    result = self._execute_task(context, task_orig)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/python3.11/lib/python3.11/site-packages/airflow/models/taskinstance.py", line 1746, in _execute_task
    result = execute_callable(context=context, **execute_callable_kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/python3.11/lib/python3.11/site-packages/airflow/providers/google/cloud/operators/dataproc.py", line 3020, in execute
    result = hook.wait_for_operation(
             ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/python3.11/lib/python3.11/site-packages/airflow/providers/google/cloud/hooks/dataproc.py", line 270, in wait_for_operation
    raise AirflowException(error)
airflow.exceptions.AirflowException: 409 Google Cloud Dataproc Agent reports job failure. If logs are available, they can be found at:
https://console.cloud.google.com/dataproc/batches/us-east4/bdc-coverage-20250217-114718?project=vz-it-np-j2lv-dev-cmido-0
gcloud dataproc batches wait 'bdc-coverage-20250217-114718' --region 'us-east4' --project 'vz-it-np-j2lv-dev-cmido-0'
https://console.cloud.google.com/storage/browser/dataproc-staging-us-east4-878261929676-qkkbu0xx/google-cloud-dataproc-metainfo/cad32c6e-9ce0-42b0-b3db-ac4a7a934c1d/jobs/srvls-batch-eaf0c2a0-7036-4c84-8245-c93ba2d33017/
gs://dataproc-staging-us-east4-878261929676-qkkbu0xx/google-cloud-dataproc-metainfo/cad32c6e-9ce0-42b0-b3db-ac4a7a934c1d/jobs/srvls-batch-eaf0c2a0-7036-4c84-8245-c93ba2d33017/driveroutput.* 10: Google Cloud Dataproc Agent reports job failure. If logs are available, they ca
[2025-02-17, 11:53:05 UTC] {} INFO - n be found at:
https://console.cloud.google.com/dataproc/batches/us-east4/bdc-coverage-20250217-114718?project=vz-it-np-j2lv-dev-cmido-0
gcloud dataproc batches wait 'bdc-coverage-20250217-114718' --region 'us-east4' --project 'vz-it-np-j2lv-dev-cmido-0'
https://console.cloud.google.com/storage/browser/dataproc-staging-us-east4-878261929676-qkkbu0xx/google-cloud-dataproc-metainfo/cad32c6e-9ce0-42b0-b3db-ac4a7a934c1d/jobs/srvls-batch-eaf0c2a0-7036-4c84-8245-c93ba2d33017/
gs://dataproc-staging-us-east4-878261929676-qkkbu0xx/google-cloud-dataproc-metainfo/cad32c6e-9ce0-42b0-b3db-ac4a7a934c1d/jobs/srvls-batch-eaf0c2a0-7036-4c84-8245-c93ba2d33017/driveroutput.*

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/python3.11/lib/python3.11/site-packages/airflow/models/taskinstance.py", line 1704, in _run_finished_callback
    callback(context)
  File "/home/airflow/gcs/dags/vz-it-j2lv-cmido-0/observability/aid_observability.py", line 28, in observability_step_email
    project_name = variables['project_name']
                   ~~~~~~~~~^^^^^^^^^^^^^^^^
KeyError: 'project_name'
[2025-02-17, 11:53:05 UTC] {standard_task_runner.py:104} ERROR - Failed to execute job 13050204 for task bdc_raw_coverage (409 Google Cloud Dataproc Agent reports job failure. If logs are available, they can be found at:
https://console.cloud.google.com/dataproc/batches/us-east4/bdc-coverage-20250217-114718?project=vz-it-np-j2lv-dev-cmido-0
gcloud dataproc batches wait 'bdc-coverage-20250217-114718' --region 'us-east4' --project 'vz-it-np-j2lv-dev-cmido-0'
https://console.cloud.google.com/storage/browser/dataproc-staging-us-east4-878261929676-qkkbu0xx/google-cloud-dataproc-metainfo/cad32c6e-9ce0-42b0-b3db-ac4a7a934c1d/jobs/srvls-batch-eaf0c2a0-7036-4c84-8245-c93ba2d33017/
gs://dataproc-staging-us-east4-878261929676-qkkbu0xx/google-cloud-dataproc-metainfo/cad32c6e-9ce0-42b0-b3db-ac4a7a934c1d/jobs/srvls-batch-eaf0c2a0-7036-4c84-8245-c93ba2d33017/driveroutput.* 10: Google Cloud Dataproc Agent reports job failure. If logs are available, they can be found at:
https://console.cloud.google.com/dataproc/batches/us-east4/bdc-coverage-20250217-114718?project=vz-it-np-j2lv-dev-cmido-0
gcloud dataproc batches wait 'bdc-coverage-20250217-114718' --region 'us-east4' --project 'vz-it-np-j2lv-dev-cmido-0'
https://console.cloud.google.com/storage/browser/dataproc-staging-us-east4-878261929676-qkkbu0xx/google-cloud-dataproc-metainfo/cad32c6e-9ce0-42b0-b3db-ac4a7a934c1d/jobs/srvls-batch-eaf0c2a0-7036-4c84-8245-c93ba2d33017/
gs://dataproc-staging-us-east4-878261929676-qkkbu0xx/google-cloud-dataproc-metainfo/cad32c6e-9ce0-42b0-b3db-ac4a7a934c1d/jobs/srvls-batch-eaf0c2a0-7036-4c84-8245-c93ba2d33017/driveroutput.*; 341049)
[2025-02-17, 11:53:05 UTC] {local_task_job_runner.py:228} INFO - Task exited with return code 1
[2025-02-17, 11:53:05 UTC] {taskinstance.py:2781} INFO - 0 downstream tasks scheduled from follow-on schedule check
