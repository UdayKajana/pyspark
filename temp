import apache_beam as beam
from apache_beam.options.pipeline_options import PipelineOptions
from apache_beam.io.gcp.pubsub import ReadFromPubSub
import apache_beam.io.gcp.spanner as sp
import json
import argparse
import logging
from typing import NamedTuple, Iterator, Optional
from datetime import datetime, timezone
from apache_beam.transforms.trigger import AfterWatermark, AfterProcessingTime, AccumulationMode, AfterCount, OrFinally
import time

class TemplateOptions(PipelineOptions):
    @classmethod
    def _add_argparse_args(cls, parser):
        parser.add_value_provider_argument('--params', type=str)

class ConvertToSpannerMutation(NamedTuple):
    telephone_number: Optional[str]
    trouble_report_number: Optional[str]
    address_id: Optional[str]
    chronic_flag: Optional[str]
    chronic_total: Optional[str]
    circuit_id: Optional[str]
    circuit_type: Optional[str]
    reported_ts: Optional[str]
    data_circuit_id: Optional[str]
    video_circuit_id: Optional[str]
    window_timestamp: Optional[str]
    insertion_timestamp: Optional[str]

beam.coders.registry.register_coder(ConvertToSpannerMutation, beam.coders.RowCoder)

class DecodeAvroRecords(beam.DoFn):
    def process(self, element, timestamp=beam.DoFn.TimestampParam) -> Iterator[dict]:
        import avro.io as avro_io
        import avro.schema
        from io import BytesIO
        import time

        raw_schema = """{"namespace": "com.vz.vznet",
                        "type": "record",
                        "name": "VznetDefault",
                        "doc": "Default schema for events in transit",
                        "fields": [
                        {"name": "timestamp", "type": "long"},
                        {"name": "host", "type": "string"},
                        {"name": "src",  "type": "string" },
                        {"name": "_event_ingress_ts", "type": "long"},
                        {"name": "_event_origin", "type": "string"},
                        {"name": "_event_tags", "type": {"type": "array", "items": "string"}},
                        {"name": "_event_route", "type": "string"},
                        {"name": "_event_metrics", "type": ["null", "bytes"], "default": null},
                        {"name": "rawdata", "type": "bytes"}
                        ]}"""
        try:
            # Handle element data whether it comes from PubSub or direct
            avro_data = element.data if hasattr(element, 'data') else element
            
            # Parse schema
            schema = avro.schema.parse(raw_schema)
            avro_reader = avro_io.DatumReader(schema)
            
            # Create decoder and read data
            avro_message = avro_io.BinaryDecoder(BytesIO(avro_data))
            message = avro_reader.read(avro_message)
            
            # Decode rawdata from bytes to string and parse as JSON
            if message['rawdata']:
                message['rawdata'] = json.loads(message['rawdata'].decode('utf-8'))
                
            # Get event timestamp
            event_time = float(message['timestamp']) / 1000.0  # Convert to seconds
            
            # Yield the decoded rawdata with timestamp
            yield beam.window.TimestampedValue(message['rawdata'], event_time)
            
        except Exception as e:
            logging.error(f"Error decoding Avro message: {str(e)}")
            logging.error(f"Element type: {type(element)}")
            logging.error(f"Element content: {element}")
            raise

class PrepareSpannerData(beam.DoFn):
    def process(self, element, window=beam.DoFn.WindowParam) -> Iterator[ConvertToSpannerMutation]:
        try:
            window_start_utc = window.start.to_utc_datetime()
            window_start_str = window_start_utc.strftime('%Y-%m-%dT%H:%M:%S.%fZ')
            mutation = self.transform_to_mutation(element)
            if mutation:
                yield mutation._replace(window_timestamp=window_start_str)
        except Exception as e:
            logging.error(f"Error processing element: {element}")
            logging.error(f"Error details: {str(e)}")
            return

    def format_date(self, value):
        if value is None:
            return None
        try:
            if isinstance(value, str):
                dt = datetime.strptime(value.split('.')[0], '%Y-%m-%d %H:%M:%S')
                return dt.strftime('%Y-%m-%dT%H:%M:%S.%fZ')
            return value
        except Exception as e:
            logging.error(f"Date formatting error: {str(e)}")
            return None

    def safe_str(self, value):
        if value is None:
            return None
        try:
            return str(value)
        except Exception:
            return None

    def transform_to_mutation(self, element: dict):
        if not isinstance(element, dict):
            logging.error(f"Expected dictionary, got {type(element)}")
            return None

        current_time = datetime.utcnow().strftime('%Y-%m-%dT%H:%M:%S.%fZ')

        # Handle direct element since we've already decoded rawdata in DecodeAvroRecords
        return ConvertToSpannerMutation(
            telephone_number=self.safe_str(element.get('telephone_number')),
            trouble_report_number=self.safe_str(element.get('trouble_report_num')),
            address_id=self.safe_str(element.get('addres1s_id')),  # Note: Matches the typo in your publisher
            chronic_flag=self.safe_str(element.get('chronic_flag')),
            chronic_total=self.safe_str(element.get('chronic1_total')),  # Note: Matches the typo in your publisher
            circuit_id=self.safe_str(element.get('line_id_trimmed')),
            circuit_type=self.safe_str(element.get('port_as1sociated_service')),  # Note: Matches the typo in your publisher
            reported_ts=self.format_date(element.get('date_opened')),
            data_circuit_id=self.safe_str(element.get('data_circuit_id')),
            video_circuit_id=self.safe_str(element.get('video_circuit_id')),
            window_timestamp=None,
            insertion_timestamp=current_time
        )

def run(known_args, beam_args):
    options = {
        'project': known_args.project,
        'runner': known_args.runner,
        'region': known_args.region,
        'staging_location': known_args.staging_location,
        'temp_location': known_args.temp_location,
        'template_location': known_args.template_location,
        'save_main_session': True,
        'streaming': True,
        'sdk_container_image': known_args.sdk_container_image,
        'sdk_location': 'container'
    }

    pipeline_options = PipelineOptions.from_dictionary(options)

    with beam.Pipeline(options=pipeline_options) as p:
        data = (p
            | "Read From Pubsub" >> ReadFromPubSub(
                subscription=f"{known_args.pubsub_subscription_name}",
                with_attributes=True,
                timestamp_attribute=None
            )
            | "Decode Avro" >> beam.ParDo(DecodeAvroRecords())
            | "Window Into Fixed Intervals" >> beam.WindowInto(
                beam.window.FixedWindows(120),
                trigger=AfterWatermark(
                    early=AfterProcessingTime(120) | AfterCount(100),
                    late=AfterCount(1)
                ),
                allowed_lateness=beam.transforms.window.Duration(seconds=300),
                accumulation_mode=AccumulationMode.ACCUMULATING
            )
            | 'Prepare Spanner Data' >> beam.ParDo(PrepareSpannerData())
            | 'Add Key' >> beam.Map(lambda x: (1, x))
            | 'Group Into Batches' >> beam.GroupByKey()
            | 'Ungroup' >> beam.FlatMap(lambda x: x[1])
        )

        _ = (data
            | "Write To Spanner" >> sp.SpannerInsertOrUpdate(
                instance_id=known_args.spanner_instance,
                database_id=known_args.spanner_dataset,
                project_id=known_args.spanner_project,
                table=known_args.spanner_table,
                expansion_service=known_args.expansion_service,
                max_number_mutations=7000,
                high_priority=True
            )
        )

def parse_arguments():
    parser = argparse.ArgumentParser()
    parser.add_argument('--project', required=True, help='GCP Project ID')
    parser.add_argument('--runner', default="DataflowRunner", help='Pipeline runner')
    parser.add_argument('--region', default="us-east4", help='GCP project region')
    parser.add_argument('--staging_location', required=True, help='Staging GCS bucket path')
    parser.add_argument('--temp_location', required=True, help='Temp GCS bucket path')
    parser.add_argument('--template_location', required=True, help='GCS bucket path to save template')
    parser.add_argument('--pubsub_subscription_name', required=True, help='Input Subscription')
    parser.add_argument('--sdk_container_image', default='us-east4-docker.pkg.dev/vz-it-np-gudv-dev-vzntdo-0/vznet/wireline:1.0.0', help='sdk_container_image location')
    parser.add_argument('--spanner_dataset', required=True, help='spanner dataset')
    parser.add_argument('--spanner_instance', required=True, help='spanner instance')
    parser.add_argument('--spanner_project', required=True, help='spanner project')
    parser.add_argument('--spanner_table', required=True, help='spanner table')
    parser.add_argument('--expansion_service', default='localhost:50768', help='Expansion service host:port')
    return parser.parse_known_args()

if __name__ == '__main__':
    logging.getLogger().setLevel(logging.INFO)
    known_args, beam_args = parse_arguments()
    run(known_args, beam_args)
    print(f'Created template at {known_args.template_location}.')
