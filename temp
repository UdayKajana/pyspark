import logging
import os
import sys
from datetime import datetime, timezone, timedelta

import google.cloud.logging
import pendulum
import yaml
from airflow import models
from airflow.operators.email import EmailOperator
from airflow.operators.empty import EmptyOperator
from airflow.operators.python import PythonOperator
from airflow.providers.google.cloud.operators.dataproc import DataprocCreateBatchOperator
from functools import partial

BASE_DIR = '/home/airflow/gcs/dags/vz-it-j2lv-cmido-0'
sys.path.append(f"{BASE_DIR}/dags/common")
task_logger = logging.getLogger('airflow.task')
client = google.cloud.logging.Client()
logger = client.logger(name="cqes_rule_based_daily")

project = os.environ['GCP_PROJECT']

# observability
MI_ABSOLUTE_PATH = r'/home/airflow/gcs/dags/vz-it-j2lv-cmido-0/observability'
sys.path.append(f"{MI_ABSOLUTE_PATH}")

from aid_observability import observability_step_email

# Read the YML file and pass the respective parameters based on the GCP project
with open(f"{BASE_DIR}/config/bdc_coverage.yaml", 'r') as file:
    data = yaml.full_load(file)

# Initialize variables dictionary with all required keys
variables = {
    'steps': {
        3: {
            'src_table': None,
            'target_table': None
        }
    },
    'opsgenie_api_key': None  # Initialize opsgenie_api_key
}

# Process YAML configuration
for key, value in data.items():
    for key1, value1 in value.items():
        if key1 == project:
            dict = value1
            for i in dict:
                project_id = i['project_id']
                JOB_NAME = i['JOB_NAME']
                PROCESS_NAME = i['PROCESS_NAME']
                dag_bucket = i['dag_bucket']
                mi_artifactory_bucket = i['mi_artifactory_bucket']
                region = i['region']
                sa_conn_id = i['sa_conn_id']
                subnet = i['subnet']
                cluster_name = i['cluster_name']
                src_script_location = i['src_script_location']
                main_script = i['main_script']
                config_location = i['config_location']
                constants = i['constants']
                functions = i['functions']
                queries = i['queries']
                spark_jar_uri = i['spark_jar_uri']
                dag_email_recipient = i['email_id']
                application_name = i['application_name']
                j2lv_service_account = i['j2lv_service_account']
                kms_key = i['kms_key']
                metastore_service = i['metastore_service']
                priority = i['priority']
                critical_priority = i['critical_priority']
                critical_day = i['critical_day']
                project_name = i['project_name']
                job_frequency = i['job_frequency']
                
                # Update variables dictionary
                variables['steps'][3]['src_table'] = i['src_table']
                variables['steps'][3]['target_table'] = i['target_table']
                variables['opsgenie_api_key'] = i['opsgenie_api_key']
            break

def failure_alert(**kwargs):
    email = EmailOperator(
        task_id="send_failed_task",
        to=dag_email_recipient,
        subject="BDC Dag Failed",
        html_content=" Task" + " Failed",
        dag=dag
    )
    email.execute(context=kwargs)

default_args = {
    'owner': 'airflow',
    'depends_on_past': False,
    "start_date": datetime(2025, 1, 24),
    'email': dag_email_recipient,
    'email_on_success': True,
    'email_on_failure': True,
    'email_on_retry': False,
    'retries': 0,
    "project_id": project_id,
    "region": region,
    'retry_delay': timedelta(minutes=10),
    'on_failure_callback': failure_alert
}

# Define history server log directory
HISTORY_SERVER_LOG_DIR = f"gs://{dag_bucket}/spark-history-logs"

# Updated Serverless Batch Configuration
BATCH_CONFIG = {
    "pyspark_batch": {
        "main_python_file_uri": f"gs://{dag_bucket}/dags/vz-it-j2lv-cmido-0/bdc_coverage/dataproc_job.py",
        "args": [
            "--frequency", "{{ params.frequency }}",
            "--backfill", "{{ params.backfill }}",
            "--backfill_start_dt", "{{ params.backfill_start_dt }}",
            "--backfill_end_dt", "{{ params.backfill_end_dt }}"
        ],
        "jar_file_uris": [spark_jar_uri] if spark_jar_uri else [],
    },
    "environment_config": {
        "execution_config": {
            "service_account": j2lv_service_account,
            "subnetwork_uri": subnet,
            "kms_key": kms_key
        },
        "peripherals_config": {
            "metastore_service": metastore_service,
            "spark_history_server_config": {
                "dataproc_cluster": f"projects/{project_id}/regions/{region}/clusters/{cluster_name}"
            }
        }
    },
    "runtime_config": {
        "version": "2.1",
        "properties": {
            "spark.driver.cores": "4",
            "spark.executor.cores": "4",
            "spark.executor.memory": "6g",
            "spark.driver.memory": "6g",
            "spark.dynamicAllocation.enabled": "true",
            "spark.dynamicAllocation.executorAllocationRatio": "0.6",
            "spark.executor.instances": "2",
            "spark.dynamicAllocation.minExecutors": "2",
            "spark.dynamicAllocation.maxExecutors": "32",
            "spark.hadoop.fs.gs.implicit.dir.repair.enable": "false",
            "spark.hadoop.google.cloud.auth.service.account.enable": "true",
            "spark.hadoop.fs.gs.project.id": project_id,
            "spark.hadoop.google.cloud.auth.service.account.json.keyfile": j2lv_service_account,

        }
    }
}

with models.DAG(
        "dg_bdc_raw_coverage",
        tags=["bdc", "coverage", "bdc_coverage"],
        default_args=default_args,
        catchup=False,
        schedule_interval=None,
        params={
            "frequency": "daily",
            "backfill": "False",
            "backfill_start_dt": "2023-12-01",
            "backfill_end_dt": "2023-12-31",
            "run_date": "dummy_date"
        }
) as dag:
    
    task_start = EmptyOperator(
        task_id='start',
        dag=dag
    )

    submit_job = DataprocCreateBatchOperator(
        task_id="bdc_raw_coverage",
        project_id=project_id,
        region=region,
        batch=BATCH_CONFIG,
        batch_id=f"bdc-coverage-{datetime.now().strftime('%Y%m%d-%H%M%S')}",
        gcp_conn_id=sa_conn_id,
        on_failure_callback=partial(observability_step_email, variables, step=3),
        on_success_callback=partial(observability_step_email, variables, step=3)
    )

    task_end = EmptyOperator(
        task_id='end',
        dag=dag,
        on_failure_callback=partial(observability_step_email, variables, step=5),
        on_success_callback=partial(observability_step_email, variables, step=5)
    )

    task_start >> submit_job >> task_end




[2025-02-17, 10:36:48 UTC] {client.py:110} INFO - OpenLineageClient will use `http` transport
[2025-02-17, 10:36:53 UTC] {adapter.py:163} WARNING - Failed to emit OpenLineage event of id 0195137b-2fe6-7c06-b2cd-9bfdc82688a2
[2025-02-17, 10:36:53 UTC] {connection.py:232} WARNING - Connection schemes (type: google_cloud_platform) shall not contain '_' according to RFC3986.
[2025-02-17, 10:36:53 UTC] {base.py:73} INFO - Using connection ID '***' for task execution.
[2025-02-17, 10:36:53 UTC] {dataproc.py:2998} INFO - Creating batch bdc-coverage-20250217-103647
[2025-02-17, 10:36:53 UTC] {dataproc.py:2999} INFO - Once started, the batch job will be available at https://console.cloud.google.com/dataproc/batches/us-east4/bdc-coverage-20250217-103647/monitoring?project=vz-it-np-j2lv-dev-cmido-0
[2025-02-17, 10:36:53 UTC] {warnings.py:110} WARNING - /opt/python3.11/lib/python3.11/site-packages/airflow/providers/openlineage/utils/utils.py:541: AirflowProviderDeprecationWarning: Call to deprecated class UnknownOperatorAttributeRunFacet. (To be removed in the next release. Make sure to use information from AirflowRunFacet instead.)
  UnknownOperatorAttributeRunFacet(

[2025-02-17, 10:36:53 UTC] {warnings.py:110} WARNING - /opt/python3.11/lib/python3.11/site-packages/airflow/providers/openlineage/plugins/adapter.py:93: DeprecationWarning: `OpenLineageClient.from_environment()` is deprecated. Use `OpenLineageClient()`.
  self._client = OpenLineageClient.from_environment()

[2025-02-17, 10:36:53 UTC] {client.py:110} INFO - OpenLineageClient will use `http` transport
[2025-02-17, 10:36:58 UTC] {adapter.py:163} WARNING - Failed to emit OpenLineage event of id 0195137b-2fe6-7c06-b2cd-9bfdc82688a2
[2025-02-17, 10:36:59 UTC] {taskinstance.py:1939} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/opt/python3.11/lib/python3.11/site-packages/google/api_core/grpc_helpers.py", line 76, in error_remapped_callable
    return callable_(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/python3.11/lib/python3.11/site-packages/grpc/_channel.py", line 1176, in __call__
    return _end_unary_response_blocking(state, call, False, None)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/python3.11/lib/python3.11/site-packages/grpc/_channel.py", line 1005, in _end_unary_response_blocking
    raise _InactiveRpcError(state)  # pytype: disable=not-instantiable
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
grpc._channel._InactiveRpcError: <_InactiveRpcError of RPC that terminated with:
	status = StatusCode.INVALID_ARGUMENT
	details = "Dataproc Cluster 'projects/vz-it-np-j2lv-dev-cmido-0/regions/us-east4/clusters/vz-it-np-j2lv-cmido-teradata' must have the property 'spark:spark.history.fs.logDirectory' configured to act as a Spark history server"
	debug_error_string = "UNKNOWN:Error received from peer ipv4:63.22.171.3:443 {grpc_message:"Dataproc Cluster \'projects/vz-it-np-j2lv-dev-cmido-0/regions/us-east4/clusters/vz-it-np-j2lv-cmido-teradata\' must have the property \'spark:spark.history.fs.logDirectory\' configured to act as a Spark history server", grpc_status:3, created_time:"2025-02-17T10:36:53.942701467+00:00"}"
>

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/python3.11/lib/python3.11/site-packages/airflow/providers/google/cloud/operators/dataproc.py", line 3005, in execute
    self.operation = hook.create_batch(
                     ^^^^^^^^^^^^^^^^^^
  File "/opt/python3.11/lib/python3.11/site-packages/airflow/providers/google/common/hooks/base_google.py", line 486, in inner_wrapper
    return func(self, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/python3.11/lib/python3.11/site-packages/airflow/providers/google/cloud/hooks/dataproc.py", line 969, in create_batch
    result = client.create_batch(
             ^^^^^^^^^^^^^^^^^^^^
  File "/opt/python3.11/lib/python3.11/site-packages/google/cloud/dataproc_v1/services/batch_controller/client.py", line 826, in create_batch
    response = rpc(
               ^^^^
  File "/opt/python3.11/lib/python3.11/site-packages/google/api_core/gapic_v1/method.py", line 131, in __call__
    return wrapped_func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/python3.11/lib/python3.11/site-packages/google/api_core/grpc_helpers.py", line 78, in error_remapped_callable
    raise exceptions.from_grpc_error(exc) from exc
google.api_core.exceptions.InvalidArgument: 400 Dataproc Cluster 'projects/vz-it-np-j2lv-dev-cmido-0/regions/us-east4/clusters/vz-it-np-j2lv-cmido-teradata' must have the property 'spark:spark.history.fs.logDirectory' configured to act as a Spark history server
[2025-02-17, 10:36:59 UTC] {taskinstance.py:1401} INFO - Marking task as FAILED. dag_id=dg_bdc_raw_coverage, task_id=bdc_raw_coverage, execution_date=20250217T103635, start_date=20250217T103648, end_date=20250217T103659
[2025-02-17, 10:36:59 UTC] {warnings.py:110} WARNING - /opt/python3.11/lib/python3.11/site-packages/airflow/utils/email.py:154: RemovedInAirflow3Warning: Fetching SMTP credentials from configuration variables will be deprecated in a future release. Please set credentials using a connection instead.
  send_mime_email(e_from=mail_from, e_to=recipients, mime_msg=msg, conn_id=conn_id, dryrun=dryrun)

[2025-02-17, 10:36:59 UTC] {configuration.py:1067} WARNING - section/key [smtp/smtp_user] not found in config
[2025-02-17, 10:36:59 UTC] {email.py:270} INFO - Email alerting: attempt 1
[2025-02-17, 10:36:59 UTC] {email.py:281} INFO - Sent an alert email to ['vuyyuri.trilok.narayana.varma@verizon.com']
[2025-02-17, 10:36:59 UTC] {taskinstance.py:1707} ERROR - Error when executing partial callback
Traceback (most recent call last):
  File "/opt/python3.11/lib/python3.11/site-packages/google/api_core/grpc_helpers.py", line 76, in error_remapped_callable
    return callable_(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/python3.11/lib/python3.11/site-packages/grpc/_channel.py", line 1176, in __call__
    return _end_unary_response_blocking(state, call, False, None)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/python3.11/lib/python3.11/site-packages/grpc/_channel.py", line 1005, in _end_unary_response_blocking
    raise _InactiveRpcError(state)  # pytype: disable=not-instantiable
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
grpc._channel._InactiveRpcError: <_InactiveRpcError of RPC that terminated with:
	status = StatusCode.INVALID_ARGUMENT
	details = "Dataproc Cluster 'projects/vz-it-np-j2lv-dev-cmido-0/regions/us-east4/clusters/vz-it-np-j2lv-cmido-teradata' must have the property 'spark:spark.history.fs.logDirectory' configured to act as a Spark history server"
	debug_error_string = "UNKNOWN:Error received from peer ipv4:63.22.171.3:443 {grpc_message:"Dataproc Cluster \'projects/vz-it-np-j2lv-dev-cmido-0/regions/us-east4/clusters/vz-it-np-j2lv-cmido-teradata\' must have the property \'spark:spark.history.fs.logDirectory\' configured to act as a Spark history server", grpc_status:3, created_time:"2025-02-17T10:36:53.942701467+00:00"}"
>

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/python3.11/lib/python3.11/site-packages/airflow/models/taskinstance.py", line 1519, in _run_raw_task
    self._execute_task_with_callbacks(context, test_mode, session=session)
  File "/opt/python3.11/lib/python3.11/site-packages/airflow/models/taskinstance.py", line 1683, in _execute_task_with_callbacks
    result = self._execute_task(context, task_orig)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/python3.11/lib/python3.11/site-packages/airflow/models/taskinstance.py", line 1746, in _execute_task
    result = execute_callable(context=context, **execute_callable_kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/python3.11/lib/python3.11/site-packages/airflow/providers/google/cloud/operators/dataproc.py", line 3005, in execute
    self.operation = hook.create_batch(
                     ^^^^^^^^^^^^^^^^^^
  File "/opt/python3.11/lib/python3.11/site-packages/airflow/providers/google/common/hooks/base_google.py", line 486, in inner_wrapper
    return func(self, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/python3.11/lib/python3.11/site-packages/airflow/providers/google/cloud/hooks/dataproc.py", line 969, in create_batch
    result = client.create_batch(
             ^^^^^^^^^^^^^^^^^^^^
  File "/opt/python3.11/lib/python3.11/site-packages/google/cloud/dataproc_v1/services/batch_controller/client.py", line 826, in create_batch
    response = rpc(
               ^^^^
  File "/opt/python3.11/lib/python3.11/site-packages/google/api_core/gapic_v1/method.py", line 131, in __call__
    return wrapped_func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/python3.11/lib/python3.11/site-packages/google/api_core/grpc_helpers.py", line 78, in error_remapped_callable
    raise exceptions.from_grpc_error(exc) from exc
google.api_core.exceptions.InvalidArgument: 400 Dataproc Cluster 'projects/vz-it-np-j2lv-dev-cmido-0/regions/us-east4/clusters/vz-it-np-j2lv-cmido-teradata' must have the property 'spark:spark.history.fs.logDirectory' configured to act as a Spark history server

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/python3.11/lib/python3.11/site-packages/airflow/models/taskinstance.py", line 1704, in _run_finished_callback
    callback(context)
  File "/home/airflow/gcs/dags/vz-it-j2lv-cmido-0/observability/aid_observability.py", line 16, in observability_step_
[2025-02-17, 10:36:59 UTC] {} INFO - email
    BASE_DIR = variables['base_dir']
               ~~~~~~~~~^^^^^^^^^^^^
KeyError: 'base_dir'
[2025-02-17, 10:36:59 UTC] {standard_task_runner.py:104} ERROR - Failed to execute job 13046653 for task bdc_raw_coverage (400 Dataproc Cluster 'projects/vz-it-np-j2lv-dev-cmido-0/regions/us-east4/clusters/vz-it-np-j2lv-cmido-teradata' must have the property 'spark:spark.history.fs.logDirectory' configured to act as a Spark history server; 215966)
[2025-02-17, 10:36:59 UTC] {local_task_job_runner.py:228} INFO - Task exited with return code 1
[2025-02-17, 10:36:59 UTC] {taskinstance.py:2781} INFO - 0 downstream tasks scheduled from follow-on schedule check
Version: 2.7.3+composer
