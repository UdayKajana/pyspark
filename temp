import logging
import os
import sys
from datetime import datetime, timedelta

import google.cloud.logging
import pendulum
import yaml
from airflow import models
from airflow.operators.email import EmailOperator
from airflow.operators.empty import EmptyOperator
from airflow.operators.python import PythonOperator
from airflow.providers.google.cloud.operators.dataproc import DataprocCreateBatchOperator
from functools import partial

BASE_DIR = '/home/airflow/gcs/dags/vz-it-j2lv-cmido-0'
sys.path.append(f"{BASE_DIR}/dags/common")
task_logger = logging.getLogger('airflow.task')
client = google.cloud.logging.Client()
logger = client.logger(name="cqes_rule_based_daily")

project = os.environ['GCP_PROJECT']

# observability
MI_ABSOLUTE_PATH = r'/home/airflow/gcs/dags/vz-it-j2lv-cmido-0/observability'
sys.path.append(f"{MI_ABSOLUTE_PATH}")

from aid_observability import observability_step_email

# Read the YML file and pass the respective parameters based on the GCP project
with open(f"{BASE_DIR}/config/bdc_coverage.yaml", 'r') as file:
    data = yaml.full_load(file)

# Initialize variables dictionary with all required keys
variables = {
    'steps': {
        3: {
            'src_table': None,
            'target_table': None
        }
    },
    'opsgenie_api_key': None,  # Initialize opsgenie_api_key
    'base_dir': BASE_DIR  # Added base_dir for observability_step_email
}

# Process YAML configuration
for key, value in data.items():
    for key1, value1 in value.items():
        if key1 == project:
            dict = value1
            for i in dict:
                project_id = i['project_id']
                JOB_NAME = i['JOB_NAME']
                PROCESS_NAME = i['PROCESS_NAME']
                dag_bucket = i['dag_bucket']
                mi_artifactory_bucket = i['mi_artifactory_bucket']
                region = i['region']
                sa_conn_id = i['sa_conn_id']
                subnet = i['subnet']
                src_script_location = i['src_script_location']
                main_script = i['main_script']
                config_location = i['config_location']
                constants = i['constants']
                functions = i['functions']
                queries = i['queries']
                spark_jar_uri = i['spark_jar_uri']
                dag_email_recipient = i['email_id']
                application_name = i['application_name']
                j2lv_service_account = i['j2lv_service_account']
                kms_key = i['kms_key']
                metastore_service = i['metastore_service']
                priority = i['priority']
                critical_priority = i['critical_priority']
                critical_day = i['critical_day']
                project_name = i['project_name']
                job_frequency = i['job_frequency']
                
                # Update variables dictionary
                variables['steps'][3]['src_table'] = i['src_table']
                variables['steps'][3]['target_table'] = i['target_table']
                variables['opsgenie_api_key'] = i['opsgenie_api_key']
            break

def failure_alert(**kwargs):
    email = EmailOperator(
        task_id="send_failed_task",
        to=dag_email_recipient,
        subject="BDC Dag Failed",
        html_content=" Task" + " Failed",
        dag=dag
    )
    email.execute(context=kwargs)

default_args = {
    'owner': 'airflow',
    'depends_on_past': False,
    "start_date": datetime(2025, 1, 24),
    'email': dag_email_recipient,
    'email_on_success': True,
    'email_on_failure': True,
    'email_on_retry': False,
    'retries': 0,
    "project_id": project_id,
    "region": region,
    'retry_delay': timedelta(minutes=10),
    'on_failure_callback': failure_alert
}

# Define history server log directory
HISTORY_SERVER_LOG_DIR = f"gs://{dag_bucket}/spark-history-logs"

# Updated Serverless Batch Configuration
BATCH_CONFIG = {
    "pyspark_batch": {
        "main_python_file_uri": f"gs://{dag_bucket}/dags/vz-it-j2lv-cmido-0/bdc_coverage/dataproc_job.py",
        "args": [
            "--frequency", "{{ params.frequency }}",
            "--backfill", "{{ params.backfill }}",
            "--backfill_start_dt", "{{ params.backfill_start_dt }}",
            "--backfill_end_dt", "{{ params.backfill_end_dt }}"
        ],
        "jar_file_uris": [spark_jar_uri] if spark_jar_uri else [],
    },
    "environment_config": {
        "execution_config": {
            "service_account": j2lv_service_account,
            "subnetwork_uri": subnet,
            "kms_key": kms_key
        },
        "peripherals_config": {
            "metastore_service": metastore_service
        }
    },
    "runtime_config": {
        "version": "2.1",
        "properties": {
            "spark.driver.cores": "4",
            "spark.executor.cores": "4",
            "spark.executor.memory": "6g",
            "spark.driver.memory": "6g",
            "spark.dynamicAllocation.enabled": "true",
            "spark.dynamicAllocation.executorAllocationRatio": "0.6",
            "spark.executor.instances": "2",
            "spark.dynamicAllocation.minExecutors": "2",
            "spark.dynamicAllocation.maxExecutors": "32",
            "spark.hadoop.fs.gs.implicit.dir.repair.enable": "false",
            "spark.hadoop.google.cloud.auth.service.account.enable": "true",
            "spark.hadoop.fs.gs.project.id": project_id,
            "spark.hadoop.google.cloud.auth.service.account.json.keyfile": j2lv_service_account,

        }
    }
}

# Remove spark history properties
if "spark_history_server_config" in BATCH_CONFIG["environment_config"]["peripherals_config"]:
    del BATCH_CONFIG["environment_config"]["peripherals_config"]["spark_history_server_config"]

with models.DAG(
        "dg_bdc_raw_coverage",
        tags=["bdc", "coverage", "bdc_coverage"],
        default_args=default_args,
        catchup=False,
        schedule_interval=None,
        params={
            "frequency": "daily",
            "backfill": "False",
            "backfill_start_dt": "2023-12-01",
            "backfill_end_dt": "2023-12-31",
            "run_date": "dummy_date"
        }
) as dag:
    
    task_start = EmptyOperator(
        task_id='start',
        dag=dag
    )

    submit_job = DataprocCreateBatchOperator(
        task_id="bdc_raw_coverage",
        project_id=project_id,
        region=region,
        batch=BATCH_CONFIG,
        batch_id=f"bdc-coverage-{datetime.now().strftime('%Y%m%d-%H%M%S')}",
        gcp_conn_id=sa_conn_id,
        on_failure_callback=partial(observability_step_email, variables, step=3),
        on_success_callback=partial(observability_step_email, variables, step=3)
    )

    task_end = EmptyOperator(
        task_id='end',
        dag=dag,
        on_failure_callback=partial(observability_step_email, variables, step=5),
        on_success_callback=partial(observability_step_email, variables, step=5)
    )

    task_start >> submit_job >> task_end

Using the default container image
Waiting for container log creation
PYSPARK_PYTHON=/opt/dataproc/conda/bin/python
JAVA_HOME=/usr/lib/jvm/temurin-17-jdk-amd64
SPARK_EXTRA_CLASSPATH=
:: loading settings :: file = /etc/spark/conf/ivysettings.xml
/opt/dataproc/conda/bin/python: can't find '__main__' module in '/var/dataproc/tmp/srvls-batch-eaf0c2a0-7036-4c84-8245-c93ba2d33017/dataproc_job.py/'


import json
import requests
import time
import os
from pathlib import Path
import pandas
import geopandas as gp
import glob
import pandas as pd
import zipfile

def getRawCoverageMetadata(input_file, output_file):
    with open(input_file, 'r') as file:
        data = json.load(file)
    filtered_data = [item for item in data['data'] if item['data_type'] == 'Mobile Broadband Raw Coverage']
    with open(output_file, 'w') as file:
        json.dump({"data": filtered_data}, file, indent=4)
    
def downloadZipFiles(file_path, zipfiles_directory, headers):
    filecount = 1
    with open(file_path, 'r') as file:
        data = json.load(file)
        for entry in data['data']:
            # Removed continue statement to process all files
            # if filecount < 124:
            #    filecount = filecount + 1
            #    continue
            entry_id = entry['id']
            file_name = entry['file_name'] + '.zip'
            url = f"https://broadbandmap.fcc.gov/nbm/map/api/getNBMDataDownloadFile/{entry_id}/2"
            print(url)
            print(file_name)
            
            response = requests.get(url, headers=headers)
            response.raise_for_status()  # Raise HTTPError for bad responses (4xx or 5xx)
            with open(os.path.join(zipfiles_directory, file_name), 'wb') as output_file:
                output_file.write(response.content)
            time.sleep(45)
            print(filecount)
            filecount = filecount + 1

def unzipFiles(directory, output_directory):
    allFiles = os.listdir(directory)
    filtered = filter(lambda singleFile: singleFile.startswith('bdc_'), allFiles)
    filtered = list(filtered)
    for file in filtered:
        if file.endswith('.zip'):
            try:
                with zipfile.ZipFile(os.path.join(directory, file), 'r') as zip_ref:
                    zip_ref.extractall(output_directory)
            except zipfile.BadZipFile:
                print(f"Skipping bad zip file: {file}")

def convertToCsv(input_directory, output_directory):
    allFiles = os.listdir(input_directory)
    filtered = filter(lambda singleFile: singleFile.startswith('bdc_'), allFiles)
    filteredFiles = list(filtered)
    for file in filteredFiles:
        try:
            gdf = gp.read_file(os.path.join(input_directory, file))
            csv_filename = file[:file.index('.')] + '.csv'
            gdf.to_csv(os.path.join(output_directory, csv_filename), index=False)
        except Exception as e:
            print(f"Error converting {file} to CSV: {e}")

from google.cloud import storage

def download_blob(bucket_name, source_blob_name, destination_file_name):
    storage_client = storage.Client()
    bucket = storage_client.bucket(bucket_name)
    blob = bucket.blob(source_blob_name)
    blob.download_to_filename(destination_file_name)
    print(
        "Downloaded storage object {} from bucket {} to local file {}.".format(
            source_blob_name, bucket_name, destination_file_name
        )
    )

from google.cloud import bigquery
import pandas as pd
def upload_toBQ(file):
    try:
        df = pd.read_csv(file,
                          dtype={'frn': str, 'providerid': str, 'mindown': str,
                                 'minup': str, 'minsignal': str, 'environmnt': str,
                                 'brandname': str, 'technology': str, 'geometry': str})
        client = bigquery.Client()
        dataset_ref = client.dataset('market_intelligence_processed')
        table_ref = dataset_ref.table('bdc_test_data1')
        job = client.load_table_from_dataframe(df, table_ref)
        job.result()  # Wait for the job to complete

        print(f"Done with {file}")
    except Exception as e:
        print(f"Error uploading {file} to BigQuery: {e}")


import os
def list_files(path):
    for filename in os.listdir(path):
        file_path = os.path.join(path, filename)
        if os.path.isfile(file_path):
            yield file_path

def pushToBQ(folder_path):
    for file_path in list_files(folder_path):
        upload_toBQ(file_path)

if __name__ == '__main__':
    import json
    headers = {
        'accept': 'application/json, text/plain, */*',
        'accept-language': 'en-US,en;q=0.9',
        'cache-control': 'no-cache',
        'pragma': 'no-cache',
        'priority': 'u=1, i',
        'referer': 'https://broadbandmap.fcc.gov/data-download/data-by-provider?version=jun2024',
        'sec-ch-ua': '"Google Chrome";v="131", "Chromium";v="131", "Not_A Brand";v="24"',
        'sec-ch-ua-mobile': '?0',
        'sec-ch-ua-platform': '"macOS"',
        'sec-fetch-dest': 'empty',
        'sec-fetch-mode': 'cors',
        'sec-fetch-site': 'same-origin',
        'user-agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36'
        }
    import shutil
    try:
        shutil.rmtree('/tmp/bdc')
    except Exception as e:
        print("File Not found!")
    import os
    os.makedirs('/tmp/bdc/zipfiles', exist_ok=True)
    os.makedirs('/tmp/bdc/gpkgfiles', exist_ok=True)
    os.makedirs('/tmp/bdc/bdc_mobile_tmobile_processed', exist_ok=True)

    zipfiles_directory = '/tmp/bdc/zipfiles'
    metadata_file = '/tmp/bdc/raw_data.json'
    raw_coverage_metadata_file = '/tmp/bdc/filtered_data.json'
    
    #Removed initial json, no need.
    #data = {}
    #with open('/tmp/bdc/raw_data.json', 'w') as f:
    #    json.dump(data, f)
    
    download_blob("us-east4-vz-it-dev-wdwg-aid-d64a7958-bucket","dags/vz-it-j2lv-cmido-0/bdc_coverage/raw_data.json","/tmp/bdc/raw_data.json")
    getRawCoverageMetadata(metadata_file, raw_coverage_metadata_file)
    downloadZipFiles(raw_coverage_metadata_file, zipfiles_directory, headers)
    gpkgfiles_directory = '/tmp/bdc/gpkgfiles'
    unzipFiles(zipfiles_directory, gpkgfiles_directory)
    csvfiles_directory = '/tmp/bdc/bdc_mobile_tmobile_processed'
    convertToCsv(gpkgfiles_directory, csvfiles_directory)
    pushToBQ(csvfiles_directory)
the error is Job failed with message [requests.exceptions.ConnectionError: HTTPSConnectionPool(host='broadbandmap.fcc.gov', port=443): Max retries exceeded with url: /nbm/map/api/getNBMDataDownloadFile/892184/2 (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7f0ae7e7bb10>: Failed to establish a new connection: [Errno 101] Network is unreachable'))]. Additional details can be found at: https://console.cloud.google.com/dataproc/batches/us-east4/bdc-coverage-20250217-122637?project=vz-it-np-j2lv-dev-cmido-0 gcloud dataproc batches wait 'bdc-coverage-20250217-122637' --region 'us-east4' --project 'vz-it-np-j2lv-dev-cmido-0' https://console.cloud.google.com/storage/browser/dataproc-staging-us-east4-878261929676-qkkbu0xx/google-cloud-dataproc-metainfo/9e790309-ee10-49a0-91ea-55f376285a23/jobs/srvls-batch-991f9d5a-cf45-45c1-914b-dbab5302f800/ gs://dataproc-staging-us-east4-878261929676-qkkbu0xx/google-cloud-dataproc-metainfo/9e790309-ee10-49a0-91ea-55f376285a23/jobs/srvls-batch-991f9d5a-cf45-45c1-914b-dbab5302f800/driveroutput.*
